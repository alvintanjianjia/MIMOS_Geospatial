{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python Lib_TrafficFlow_Class_20210419_1648_GPU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Lib_TrafficFlow_Class_20210419_1648.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Import the libraries\n",
    "\"\"\"\n",
    "\n",
    "# simple python libraries\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import time\n",
    "# random\n",
    "import random\n",
    "# statistics\n",
    "import statistics\n",
    "import sys\n",
    "# pandas for dataframes\n",
    "import pandas as pandas\n",
    "# shapes\n",
    "import shapely as shapely\n",
    "# graph\n",
    "import networkx as networkx\n",
    "import cugraph\n",
    "import cudf\n",
    "# for map drawing\n",
    "import folium as folium\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Control variables\n",
    "\"\"\"\n",
    "FLAG_printDetail = False\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Dataframe manager class\n",
    "Manages the dataframes, allowing us to easily load dataframes for resuability\n",
    "\"\"\"\n",
    "\n",
    "class Manager_Dataframe:\n",
    "\n",
    "    def __init__(self, argv_filename=None):\n",
    "        \"\"\"\n",
    "        The init function store the name of the files to be loaded into dataframes\n",
    "            argv_filename is the filename of CSV data to be read into dataframes\n",
    "                if single filename (string) then it is a file that contain all filenames\n",
    "                if multiple filenames (dictionary) then it is already a dictionary of filenames\n",
    "        \"\"\"\n",
    "        # the attributes\n",
    "        self._filenames = {}\n",
    "        self._dataframes = {}\n",
    "        # check filenames are provided or not\n",
    "        if argv_filename is None:\n",
    "            pass\n",
    "        # if single filename, it is the file that contains all of the filenames\n",
    "        elif isinstance(argv_filename, str):\n",
    "            self._filenames = load_file(\n",
    "                argv_filename=argv_filename,\n",
    "                argv_type=\"dictionary\",\n",
    "                argv_separator=\",\"\n",
    "                )\n",
    "        # if it is a dictionary of filenames\n",
    "        elif isinstance(argv_filename, dict):\n",
    "            for current_key in argv_filename:\n",
    "                self._filenames[current_key] = argv_filename[current_key]\n",
    "\n",
    "    def load_dataframes(self, argv_dropNA=False):\n",
    "        \"\"\"\n",
    "        Load dataframe from the filenames, into as an attribute of the manager class\n",
    "            argv_dropNA is the flag to determine if missing values would be dropped wjhen the dataframe is read\n",
    "        \"\"\"\n",
    "        # check if there is data to load\n",
    "        if len(self._filenames) == 0:\n",
    "            return False\n",
    "        # load in the data\n",
    "        self._dataframes = {}\n",
    "        for current_fileID in self._filenames:\n",
    "            current_filename = self._filenames[current_fileID]\n",
    "            current_dataframe = load_into_dataframe(\n",
    "                argv_filename=current_filename,\n",
    "                argv_dropNA=argv_dropNA\n",
    "                )\n",
    "            self._dataframes[current_fileID] = current_dataframe\n",
    "\n",
    "    def get_dataframe(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Return the dataframe of choice\n",
    "            argv_id is the dataframe ID to retrieve\n",
    "                if left as None, retrieve all of the dataframes (dictionary)\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            return self._dataframes\n",
    "        if argv_id in self._dataframes:\n",
    "            return self._dataframes[argv_id]\n",
    "        return None\n",
    "\n",
    "    def print_filenames(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Print the filenames\n",
    "            argv_id is the dataframe ID to retrieve the filename which generate the dataframe\n",
    "                if left as None, print out all of the dataframe filenames\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            for current_id in self._filenames:\n",
    "                current_filename = self._filenames[current_id]\n",
    "                print(str(current_id) + \": \" + str(current_filename))\n",
    "        elif argv_id in self._filenames[argv_id]:\n",
    "            current_filename = self._filenames[argv_id]\n",
    "            print(str(argv_id) + \": \" + str(current_filename))\n",
    "\n",
    "    def print_dataframes(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Print the dataframe\n",
    "            argv_id is the dataframe ID to print out the details (summary information)\n",
    "                if left as None, would print out all of the dataframes information\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            for current_id in self._dataframes:\n",
    "                print(current_id)\n",
    "                current_dataframe = self._dataframes[current_id]\n",
    "                print(current_dataframe.info())\n",
    "                print(current_dataframe.head())\n",
    "        elif argv_id in self._dataframes:\n",
    "            print(argv_id)\n",
    "            current_dataframe = self._dataframes[argv_id]\n",
    "            print(current_dataframe.info())\n",
    "            print(current_dataframe.head())\n",
    "\n",
    "    def split_dataframe(self, argv_id=None, argv_col=\"\", argv_inplace=False):\n",
    "        \"\"\"\n",
    "        Split the single dataframe into multiple dataframes based on unique values in the column\n",
    "            argv_id is the dataframe ID to be splitted\n",
    "                if left as None, the last dataframe would be splitted (just being lazy for single dataframes)\n",
    "            argv_col is the column name where the datagrame would be splitted accordingly\n",
    "                split using the helper function split_dataframes_byCol()\n",
    "                will split based on unique value in the column\n",
    "            argv_inplace is the boolean value towards replacing all of the dataframes with the splitted one\n",
    "                default value is False\n",
    "        Returns True if process is successul\n",
    "        \"\"\"\n",
    "        # get the dataframe based on the ID\n",
    "        current_dataframe = None\n",
    "        if argv_id is None:\n",
    "            for current_id in self._dataframes:\n",
    "                current_dataframe = self._dataframes[current_id]\n",
    "        elif argv_id in self._dataframes:\n",
    "            current_dataframe = self._dataframes[argv_id]\n",
    "        else:\n",
    "            return False\n",
    "        # split the dataframe        \n",
    "        current_dfs = split_dataframes_byCol(argv_df=current_dataframe.to_pandas(), argv_col=argv_col)\n",
    "        # check if it is in-place\n",
    "        if argv_inplace:\n",
    "            self._dataframes = current_dfs\n",
    "        else:\n",
    "            for current_id in current_dfs:\n",
    "                self._dataframes[current_id] = current_dfs[current_id]\n",
    "        # done\n",
    "        return True\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Graph class for layered graph\n",
    "\"\"\"\n",
    "\n",
    "class Graph_Layered:\n",
    "    \n",
    "    def __init__(self, argv_dataframe=None, argv_source=\"\", argv_destination=\"\", argv_weight=[]):\n",
    "        \"\"\"\n",
    "        Scope this down to only simple graph with single weight\n",
    "            argv_dataframe is the dataframe to be used for the layered graph\n",
    "            argv_source is the colname that would be used as the source of the edges for the graph\n",
    "            argv_destination is the colname that would be used as the destination of the edges for the graph\n",
    "            argv_weight is a list of colnames to be used as the weights for the edges of the graph\n",
    "                This is written to be more generic but in general we would be using single weighted edge\n",
    "        \"\"\"\n",
    "        # the attributes\n",
    "        self._dataframe = argv_dataframe\n",
    "        self._source = argv_source\n",
    "        self._destination = argv_destination\n",
    "        self._weight = argv_weight\n",
    "        self._graph = None\n",
    "        self._intermediates_vertices = [[]]\n",
    "        self._intermediates_weights = []\n",
    "\n",
    "    def get_stat_weights(self, argv_weight=None):\n",
    "        \"\"\"\n",
    "        Return the statistics for the edge weight\n",
    "        This is useful for the experiment variants of different weight values\n",
    "            argv_weight is the colname for the weight which the statistics is requested for\n",
    "        Return the statistics in a dictionary with keys\n",
    "            min\n",
    "            max\n",
    "            mean\n",
    "            median\n",
    "            zero\n",
    "        \"\"\"\n",
    "        # if no weight variable is selected then just use the first weight value\n",
    "        if argv_weight is None:\n",
    "            # get the stats\n",
    "            current_values = self._dataframe[self._weight[0]].tolist()\n",
    "            return_min = min(current_values)\n",
    "            return_max = max(current_values)\n",
    "            return_mean = sum(current_values) / max(len(current_values),1)\n",
    "            return_median = statistics.median(current_values)\n",
    "            # put the results in a dictionary\n",
    "            return_stats = {}\n",
    "            return_stats[\"min\"] = return_min\n",
    "            return_stats[\"max\"] = return_max\n",
    "            return_stats[\"mean\"] = return_mean\n",
    "            return_stats[\"median\"] = return_median\n",
    "            return_stats[\"zero\"] = 0\n",
    "        else:\n",
    "            # get the stats\n",
    "            current_values = self._dataframe[argv_weight].tolist()\n",
    "            return_min = min(current_values)\n",
    "            return_max = max(current_values)\n",
    "            return_mean = sum(current_values) / max(len(current_values),1)\n",
    "            return_median = statistics.median(current_values)\n",
    "            # put the results in a dictionary\n",
    "            return_stats = {}\n",
    "            return_stats[\"min\"] = return_min\n",
    "            return_stats[\"max\"] = return_max\n",
    "            return_stats[\"mean\"] = return_mean\n",
    "            return_stats[\"median\"] = return_median\n",
    "            return_stats[\"zero\"] = 0\n",
    "        return return_stats\n",
    "\n",
    "    def generate_graph(self, argv_directed=True):\n",
    "        \"\"\"\n",
    "        Generate a graph from the given dataframe, storing it within the graph object\n",
    "            argv_directed is the boolean to determine if the graph is directed or not\n",
    "        \"\"\"\n",
    "        self._graph = generate_graph(\n",
    "            argv_df=self._dataframe,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_edge=self._weight, \n",
    "            argv_directed=argv_directed)\n",
    "\n",
    "    def generate_layer_graph(self, argv_intermediates_vertices=[[]], argv_intermediates_weights=[]):\n",
    "        \"\"\"\n",
    "        Generate a layered graph from the dataframe, using the information stored within the attributes\n",
    "        The layered graph is generated through a dataframe approach (pandas) before being fed directly into networkx for the graph\n",
    "        Note graph is always directed for the layered graph approach\n",
    "            argv_intermediates_vertices is the list of vertices (vertex ID) to be used as intermediate vertices\n",
    "            argv_intermediates_weights is the list of weights (numeric) to be used for the edges connecting these intermediate vertices\n",
    "        \"\"\"\n",
    "        # update the attributes first\n",
    "        self._intermediates_vertices = argv_intermediates_vertices\n",
    "        self._intermediates_weights = argv_intermediates_weights\n",
    "        # generate the layers, store within dataframe\n",
    "        current_df_layered = generate_graph_layers_different(\n",
    "            argv_df=self._dataframe,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_weight=self._weight[0], \n",
    "            argv_intermediates_vertices=argv_intermediates_vertices, argv_intermediates_weights=argv_intermediates_weights,\n",
    "            argv_type=\"dataframe\"\n",
    "            )\n",
    "        # generate the graph\n",
    "        self._graph = generate_graph(\n",
    "            argv_df=current_df_layered,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_edge=self._weight, \n",
    "            argv_directed=True)\n",
    "\n",
    "    def print_graph_details(self):\n",
    "        \"\"\"\n",
    "        Simple function to print graph information, useful for future experiments when we update it to return values\n",
    "        \"\"\"\n",
    "        print(\"Nodes: \" + str(self._graph.number_of_nodes()))\n",
    "        print(\"Edges: \" + str(self._graph.number_of_edges()))\n",
    "        try:\n",
    "            print(\"Density: \" + str(cugraph.density(self._graph)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def run_dijsktra(self, argv_start=\"\", argv_end=\"\"):\n",
    "        \"\"\"\n",
    "        Function that runs Dijkstra from the start to the end point, if route exist\n",
    "        Uses the run_dijkstra() helper function\n",
    "        Arguments\n",
    "            argv_start is the vertexID (string) for the starting point of path\n",
    "            argv_end is the vertexID (string) for the ending point of the path\n",
    "        Returns\n",
    "            return_distance return the total distance from the start to the end\n",
    "            return_path_nodes is the list of nodes (in the graph); not the actual route for the map because there is a detailed route between vertexA and vertexB\n",
    "        Throws exception if there is no route\n",
    "        \"\"\"\n",
    "        return_distance, return_path_nodes, timer_dijkstra, timer_pathbuilding = run_dijkstra(\n",
    "            argv_graph=self._graph,\n",
    "            argv_source=str(argv_start) + \"_0\",\n",
    "            argv_target=str(argv_end) + \"_\" + str(len(self._intermediates_vertices)),\n",
    "            argv_weight_edge=self._weight[0]\n",
    "        )\n",
    "        return return_distance, return_path_nodes, timer_dijkstra, timer_pathbuilding\n",
    "\n",
    "    def build_dict_routes(self, argv_start=\"\", argv_end=\"\", argv_geom=\"\"):\n",
    "        \"\"\"\n",
    "        Function that would build a dictionary of routes (the geom) from the dataframe\n",
    "        This dictionary would be useful in reconstructing the entire complete route from the path of points; in order to visualize it on the map.\n",
    "            argv_start is the colname of the source vertex ID in the graph\n",
    "            argv_end is the colname of the destination vertex ID in the graph\n",
    "            argv_geom is th colname where the geometry of the path from the source to the destination vertex\n",
    "        Return\n",
    "            dict_routes is the dictionary of routes with key = concat(start, \" \", end) and value is the list of path\n",
    "        \"\"\"\n",
    "        # get the values from the df\n",
    "        current_geometries = self._dataframe[argv_geom].tolist()\n",
    "        current_starts = self._dataframe[argv_start].tolist()\n",
    "        current_ends = self._dataframe[argv_end].tolist()\n",
    "        # add them into dictionary if unique\n",
    "        dict_routes = {}\n",
    "        for i in range(len(current_geometries)):\n",
    "            current_geometry = current_geometries[i]\n",
    "            current_start = current_starts[i]\n",
    "            current_end = current_ends[i]\n",
    "            # use this as \n",
    "            current_point = \" \".join([current_start, current_end])\n",
    "            # add to dictionary\n",
    "            if current_point not in dict_routes:\n",
    "                dict_routes[current_point] = current_geometry\n",
    "        # return the dictionary for use\n",
    "        return dict_routes\n",
    "\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "File IO\n",
    "\"\"\"\n",
    "\n",
    "#%% Set CWD\n",
    "def set_CWD():\n",
    "    \"\"\"\n",
    "    Change to CWD to where the file is located\n",
    "    This is generally not required except for certain Python setups\n",
    "    \"\"\"\n",
    "    abspath = os.path.abspath(__file__)\n",
    "    dname = os.path.dirname(abspath)\n",
    "    os.chdir(dname)\n",
    "    if FLAG_printDetail:\n",
    "        print(os.getcwd())\n",
    "\n",
    "def load_file(argv_filename=\"\", argv_type=\"list\", argv_separator=None):\n",
    "    \"\"\"\n",
    "    Load a file and return its content\n",
    "        argv_filename is the filename for the file to be loaded\n",
    "        argv_type (string) denote the datastructure that the file would be loaded into\n",
    "    Return\n",
    "        current_list is a list of content, each line is a list item\n",
    "        current_dictionary is a dictioanry of content, with the first token as the key and subsequent tokens as a string\n",
    "    \"\"\"\n",
    "    file_data = open(argv_filename, \"r\")\n",
    "    # load into list line by line, after striping\n",
    "    if argv_type == \"list\":\n",
    "        current_list = []\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_list.append(current_line)\n",
    "        file_data.close()\n",
    "        return current_list \n",
    "    # load into dictionary, with first split as the \n",
    "    if argv_type == \"dictionary\":\n",
    "        current_dictionary = {}\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            current_dictionary[current_line[0]] = \" \".join(current_line[1:])\n",
    "        file_data.close()\n",
    "        return current_dictionary\n",
    "\n",
    "def load_dataset(argv_filename=\"\", argv_separator=\",\", argv_type=\"dictionary\", argv_name=False):\n",
    "    \"\"\"\n",
    "    Function to load dataset into a different datatypes\n",
    "    This is a more complete function than the one above, but not being used as it is not needed\n",
    "    \"\"\"\n",
    "    file_data = open(argv_filename, \"r\")\n",
    "    if argv_type == \"dictionary\":\n",
    "        current_dictionary = {}\n",
    "        counter_line = 0\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            if argv_name is False:\n",
    "                current_dictionary[\"point_\"+str(counter_line)] = \" \".join(current_line)\n",
    "            else:\n",
    "                current_dictionary[current_line[0]] = \" \".join(current_line[1:])\n",
    "            counter_line += 1\n",
    "        file_data.close()\n",
    "        return current_dictionary\n",
    "    elif argv_type == \"list\":\n",
    "        current_list = []\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            current_list.append(\" \".join(current_line))\n",
    "        file_data.close()\n",
    "        return current_list\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Dataframes\n",
    "with pandas\n",
    "skip geopandas for now, as it isn't available on the server\n",
    "\"\"\"\n",
    "\n",
    "def load_into_dataframe(argv_filename=\"\", argv_pandas_type=None, argv_pandas_geomname=\"geometry\", argv_dropNA=True):\n",
    "    \"\"\"\n",
    "    Load the filename into a dataframe\n",
    "        argv_filename is the filename for the CSV file to be read into a Pandas DF\n",
    "        argv_pandas_type is the type of Pandas we would be using such as geopandas.\n",
    "            If None, then would be default panda.\n",
    "        argv_pandas_geomname is the colname for the column with the GIS data when using GeoPandas\n",
    "        argv_dropNA is the boolean variable to determine if missing values are dropped\n",
    "            Would use the default dropNA(), future extension here could provide more flexible drops\n",
    "    Would always skip blank lines by default.\n",
    "    Return\n",
    "        return_dataframe is the pandas DF object\n",
    "    \"\"\"\n",
    "    if argv_pandas_type is None:\n",
    "        return_dataframe = pandas.read_csv(filepath_or_buffer=argv_filename, skip_blank_lines=True)\n",
    "        return_dataframe = cudf.DataFrame.from_pandas(return_dataframe)\n",
    "    elif argv_pandas_type == \"geopandas\":\n",
    "        # no need for geopandas, thus won't be included for now\n",
    "        # return_dataframe = geopandas.read_file(filename=argv_filename, GEOM_POSSIBLE_NAMES=argv_pandas_geomname, skip_blank_lines=True)\n",
    "        pass\n",
    "    # ignore missing (complete one by default)\n",
    "    if argv_dropNA:\n",
    "        return_dataframe = return_dataframe.dropna()\n",
    "    # return the dataframe\n",
    "    return return_dataframe\n",
    "\n",
    "def load_into_dataframes(argv_dict_filenames={}, argv_pandas_type=None, argv_pandas_geomname=\"geometry\", argv_dropNA=True):\n",
    "    \"\"\"\n",
    "    Load the filenames into a dictionary of dataframes\n",
    "        argv_dict_filenames is a dictionary of filenamnes to be loaded into the dataframe\n",
    "        argv_pandas_type is the type (string) of pandas to use (normal vs geopandas)\n",
    "        argv_pandas_geomname is the colname for the geometry column for geopandas\n",
    "        argv_dropNA is a boolean variable to control if rows with missing values would be dropped\n",
    "    Return\n",
    "        return_dict_dfs a dictionary of dataframes loaded from the files\n",
    "    \"\"\"\n",
    "    return_dict_dfs = {}\n",
    "    for current_key in argv_dict_filenames.keys():\n",
    "        current_filename = argv_dict_filenames[current_key]\n",
    "        return_dict_dfs[current_key] = load_into_dataframe(\n",
    "            argv_filename=current_filename,\n",
    "            argv_dropNA=argv_dropNA)\n",
    "    return return_dict_dfs\n",
    "\n",
    "def concat_dataframes(argv_dfs=[], argv_duplicates=True, argv_ignore_index=True):\n",
    "    \"\"\"\n",
    "    Concat dataframes together\n",
    "        argv_dfs is a list of dataframes\n",
    "        argv_duplicates check if duplicates are allowed, a boolean\n",
    "        argv_ignore_index is a boolean to ignore the index\n",
    "    Return\n",
    "        return_df the concatenated dataframe\n",
    "    \"\"\"\n",
    "    return_df = pandas.concat(argv_dfs, ignore_index=argv_ignore_index)\n",
    "    if not argv_duplicates:\n",
    "        return_df = return_df.drop_duplicates(keep=\"first\")\n",
    "    return return_df\n",
    "\n",
    "def split_dataframes_byCol(argv_df, argv_col=\"\"):\n",
    "    \"\"\"\n",
    "    Split a dataframe into multiple based on the unique values\n",
    "        argv_df is the dataframe object which would be used for splitting\n",
    "        argv_col is the column name in the dataframe of the object\n",
    "    Return\n",
    "        return_dfs a dictionary of Dataframes splitted based on their unique value (as the key for the dictionary as well)\n",
    "    \"\"\"\n",
    "    return_dfs = {}\n",
    "    # get unique values\n",
    "    current_values_unique = argv_df[argv_col].unique()\n",
    "    # loop through the unique values\n",
    "    for current_value in current_values_unique:\n",
    "        return_dfs[current_value] = argv_df.loc[argv_df[argv_col] == current_value]\n",
    "    return return_dfs\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Graph and graph layers\n",
    "\"\"\"\n",
    "\n",
    "def generate_graph(argv_df, argv_source=\"\", argv_destination=\"\", argv_edge=[], argv_directed=True):\n",
    "    \"\"\"\n",
    "    Generate a graph with the edges weighted according to a list of values from a dataframe\n",
    "        argv_df is the dataframe object where the graph would be generated\n",
    "        argv_source is the colname as the source for all of the edges in the graph\n",
    "        argv_destination is the colname as the destination for all of the edges in the graph\n",
    "        argv_edge is a list of colnames for edges value (multi edges are supported)\n",
    "        argv_directed is a boolean to determined if the fraph is directed.\n",
    "            Graph generated is directed by default for our use case.\n",
    "    Return\n",
    "        return_graph is a NetworkX graph object\n",
    "    \"\"\"\n",
    "    return_graph = None\n",
    "    if argv_directed:\n",
    "        return_graph = cugraph.from_cudf_edgelist(\n",
    "            df=cudf.DataFrame.from_pandas(argv_df),\n",
    "            source=argv_source,\n",
    "            destination=argv_destination,\n",
    "            edge_attr=argv_edge,\n",
    "            create_using=cugraph.DiGraph\n",
    "        )\n",
    "    #kl_selangor_graph_distance.from_cudf_edgelist(edge_df_distance, source=0, destination=1, edge_attr=2)    \n",
    "    \n",
    "    else:\n",
    "        return_graph = cugraph.from_cudf_edgelist(\n",
    "            df=cudf.DataFrame.from_pandas(argv_df),\n",
    "            source=argv_source,\n",
    "            destination=argv_destination,\n",
    "            edge_attr=argv_edge\n",
    "        )\n",
    "    #print(return_graph)\n",
    "    return return_graph\n",
    "\n",
    "def generate_graph_layers_different(\n",
    "    argv_df, argv_source=\"\", argv_destination=\"\", argv_weight=\"\", \n",
    "    argv_intermediates_vertices=[[]], argv_intermediates_weights=[],\n",
    "    argv_type=\"dataframe\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generated layered graph\n",
    "        argv_source/ argv_destination/ argv_weight are the edge details\n",
    "        argv_intermediates_vertices/ argv_intermediates_weights are array of\n",
    "            intermediate vertices [[v1,v2,v3], [v2,v5,v10,v11]]\n",
    "            weights connecting the intermediate nodes [5,2]\n",
    "        there should be at least 2 layers (1 intermediate link)\n",
    "        argv_type is how we do the layered, either via graph or dataframe\n",
    "            for now only dataframe as it is easier and more efficient\n",
    "    Return\n",
    "        return_df is the dataframe of the layered graph \n",
    "    \"\"\"\n",
    "    # for dataframe\n",
    "    if argv_type == \"dataframe\":\n",
    "        current_u = argv_df[argv_source].tolist()\n",
    "        current_v = argv_df[argv_destination].tolist()\n",
    "        current_w = argv_df[argv_weight].tolist()\n",
    "        current_u_new = []\n",
    "        current_v_new = []\n",
    "        current_w_new = []\n",
    "        # duplicate it for multiple layers first\n",
    "        current_count_layers = len(argv_intermediates_vertices)\n",
    "        # note: the +1 is needed as k intermediate layer would have k layers total\n",
    "        for count_layer in range(current_count_layers+1):\n",
    "            for count_row in range(len(current_u)):\n",
    "                current_u_new.append(str(current_u[count_row]) + \"_\" + str(count_layer))\n",
    "                current_v_new.append(str(current_v[count_row]) + \"_\" + str(count_layer))\n",
    "                current_w_new.append(float(current_w[count_row]))\n",
    "        # add in the intermediate laters\n",
    "        for count_layer in range(current_count_layers):\n",
    "            current_intermediate_vertices = argv_intermediates_vertices[count_layer]\n",
    "            current_intermediate_weight = argv_intermediates_weights[count_layer]\n",
    "            for current_intermeidate_vertex in current_intermediate_vertices:\n",
    "                current_u_new.append(str(current_intermeidate_vertex) + \"_\" + str(count_layer))\n",
    "                current_v_new.append(str(current_intermeidate_vertex) + \"_\" + str(count_layer+1))\n",
    "                current_w_new.append(float(current_intermediate_weight))\n",
    "        # create the df\n",
    "        return_df = pandas.DataFrame(\n",
    "            data=list(zip(current_u_new, current_v_new, current_w_new)),\n",
    "            columns=[argv_source, argv_destination, argv_weight]\n",
    "        )\n",
    "        #print(return_df)\n",
    "        return return_df\n",
    "    # return nothing when the wrong type is selected\n",
    "    return None\n",
    "\n",
    "def run_dijkstra(argv_graph, argv_source, argv_target, argv_weight_edge):\n",
    "    \"\"\"\n",
    "    Run dijkstra\n",
    "        argv_graph is the networkx graph to run Dijkstra on\n",
    "        argv_source and argv_target are vertex IDs\n",
    "        argv_weight_edge is the variable used for the graph weight in Dijkstra\n",
    "            Note graph could be multi edge, thus we would want to specific what is the weight we are being minimum\n",
    "    \"\"\"\n",
    "    # start timer to track SSSP runtime\n",
    "    run_timer()\n",
    "    distances_distance_table = cugraph.shortest_path(argv_graph, argv_source)\n",
    "    # stop the timer, to track SSSP runtime\n",
    "    timer_dijkstra = run_timer()\n",
    "\n",
    "    # start timer to track the path reconstruction runtime\n",
    "    run_timer()\n",
    "    distances_distance = distances_distance_table.to_pandas()    \n",
    "    current_distance = distances_distance[distances_distance['vertex'] == argv_target]['distance'].values[0]    \n",
    "    #print(distances_distance[distances_distance['vertex'] == argv_target]['predecessor'].values, 'here 2')\n",
    "    #print(distances_distance[distances_distance['vertex'] == argv_target]['predecessor'].values[0], 'here 3')    \n",
    "    distances_distance_path = []\n",
    "    distances_distance_path.append(argv_target)    \n",
    "    if current_distance < sys.float_info.max:\n",
    "        #print('start')\n",
    "        while argv_target != argv_source:            \n",
    "            #print(distances_distance_path)\n",
    "            #print(argv_target, 'argv_target')\n",
    "            #print(distances_distance[distances_distance['vertex'] == argv_target]['predecessor'].values,'dun match ur lanjiao')\n",
    "            argv_target = distances_distance[distances_distance['vertex'] == argv_target]['predecessor'].values[0]\n",
    "            distances_distance_path.append(argv_target)\n",
    "    # stop the timer for SSSP runtime\n",
    "    timer_pathbuilding = run_timer()\n",
    "    #print('end')\n",
    "    #print(current_distance, distances_distance_path, 'current distance, current_path')\n",
    "    #current_distance, current_path = cugraph.sssp(\n",
    "    #    G=argv_graph,\n",
    "    #    source=argv_source,\n",
    "    #    target=argv_target,\n",
    "    #    weight=argv_weight_edge\n",
    "    #)    \n",
    "    #print(current_distance, 'current_distance')\n",
    "    #print(distances_distance_path)    \n",
    "    return current_distance, distances_distance_path, timer_dijkstra, timer_pathbuilding\n",
    "\n",
    "def clean_layered_path(argv_path, argv_df_path={}):\n",
    "    \"\"\"\n",
    "    Function to pre-processed the returned path from Dijkstra, completing the path for visualization on the map\n",
    "        argv_path is the path from Dijsktra through a layered graph; the vertices would have the \"_level\" in the ID\n",
    "        argv_df_path is the dictionary where the concat(start, \" \", end) as the key and the full path coordinates as the value\n",
    "    Return\n",
    "        return_path_full is a string of coordinates that acts as the full path of map plotting (with , as separator).\n",
    "        return_detour is a list of coordinates that are the detour points, this helps us mark the detour points on the map\n",
    "        return_detour_long/ return_detour_lat is the same, but just broken down for long and lat for visualization\n",
    "    \"\"\"\n",
    "    # clean\n",
    "    return_path = []\n",
    "    return_detour = []\n",
    "    return_detour_long = []\n",
    "    return_detour_lat = []\n",
    "    for current_point in argv_path:\n",
    "        current_point = current_point.split(\"_\")[0]\n",
    "        return_path.append(current_point)\n",
    "    # loop through\n",
    "    return_path_full = []\n",
    "    for i in range(len(return_path)-1):\n",
    "        current_point_start = return_path[i]\n",
    "        current_point_end = return_path[i+1]\n",
    "        # if start and end the same (ie moving through layers)\n",
    "        if current_point_start == current_point_end:\n",
    "            return_detour.append(current_point_start)\n",
    "            current_point_start_tokens = current_point_start.split()\n",
    "            return_detour_long.append(str(current_point_start_tokens[0]))\n",
    "            return_detour_lat.append(str(current_point_start_tokens[1]))\n",
    "            continue\n",
    "        # continue to the next one\n",
    "        current_endpoint = current_point_start + \" \" + current_point_end\n",
    "        # print(current_endpoint)\n",
    "        # print(argv_df_path[current_endpoint])\n",
    "        \"\"\"\n",
    "        if current_endpoint in argv_df_path:\n",
    "            current_path = argv_df_path[current_endpoint]\n",
    "            current_path = current_path.replace(\"], \", \"] , \")\n",
    "            current_path = current_path.split(\" , \")\n",
    "            current_path[0] = current_path[0].replace(\"[[\", \"[\")\n",
    "            # remove the end point, except for the last one as they do repeat\n",
    "            if i != len(return_path)-2:\n",
    "                current_path.pop(-1)\n",
    "            else:\n",
    "                current_path[-1] = current_path[-1].replace(\"]]\", \"]\")\n",
    "            # current_path = current_path.split(\", \")\n",
    "            # print(current_path)\n",
    "            return_path_full.extend(current_path)\n",
    "        else:\n",
    "            raise Exception(\"Error: No path\")\n",
    "        \"\"\"\n",
    "    # return the values, concat the path\n",
    "    return_path_full = \", \".join(return_path_full)\n",
    "    return return_path_full, return_detour, return_detour_long, return_detour_lat\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Random functions\n",
    "Mainly used for evaluations\n",
    "\"\"\"\n",
    "\n",
    "def set_random_seed(argv_seed=1234):\n",
    "    \"\"\"\n",
    "    Simple function to reset the seeds\n",
    "        argv_seed is the seed value\n",
    "    \"\"\"\n",
    "    random.seed(argv_seed)\n",
    "\n",
    "def select_random(argv_values, argv_count=10):\n",
    "    \"\"\"\n",
    "    Randomly select from a list of values\n",
    "    Note that repetitions are not allowed\n",
    "        argv_values is a list of values to be selected\n",
    "        argv_count is the total number of items to be selected\n",
    "        precondition that argv_count <= argv_values\n",
    "    Return\n",
    "        return_selected is a list of randomly selected value\n",
    "    \"\"\"\n",
    "    return_selected = []\n",
    "    while len(return_selected) < argv_count:\n",
    "        current_value = random.choice(argv_values)\n",
    "        if current_value in return_selected:\n",
    "            continue\n",
    "        return_selected.append(current_value)\n",
    "    return return_selected\n",
    "\n",
    "def generate_random_lists(argv_values, argv_counts=[], argv_unique=False):\n",
    "    \"\"\"\n",
    "    From the provided list, generate multiple lists from the given sizes\n",
    "        argv_values is a list of values to be selected\n",
    "        argv_counts is a list of sizes of the lists to be generated\n",
    "        argv_unique is a boolean variable to ensure that the items in the randomly generated lists are all unique\n",
    "    Return\n",
    "        return_lists is a list of random lists generated based on the criteria\n",
    "    \"\"\"\n",
    "    return_lists = []\n",
    "    current_values = argv_values.copy()\n",
    "    # build the list for each size\n",
    "    for current_count in argv_counts:\n",
    "        # select random item based on size\n",
    "        current_list = select_random(argv_values=current_values, argv_count=current_count)\n",
    "        if argv_unique:\n",
    "            # then remove from original list\n",
    "            for current_item in current_list:\n",
    "                current_values.remove(current_item)\n",
    "        return_lists.append(current_list)\n",
    "    # return it\n",
    "    return return_lists\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Timer functions\n",
    "\"\"\"\n",
    "\n",
    "# import timer for benchmarking\n",
    "import time\n",
    "# have the start time as the global so we can make comparison\n",
    "START_TIME = time.time()\n",
    "def run_timer():\n",
    "    \"\"\"\n",
    "    Function that return the different in time between now and the previous time\n",
    "    Return\n",
    "        return_time is the time difference between now and the previous time\n",
    "    \"\"\"\n",
    "    global START_TIME\n",
    "    return_time = time.time() - START_TIME\n",
    "    START_TIME = time.time()\n",
    "    return return_time\n",
    "\n",
    "import datetime\n",
    "def get_current_timestamp():\n",
    "    \"\"\"\n",
    "    Gets the current time and format into a timestamp format\n",
    "    This is useful for auto-naming filename\n",
    "    Return\n",
    "        return_timestamp is the current timestamp formatted in YYYYMMDD_HHmm format\n",
    "    \"\"\"\n",
    "    current_timestamp = datetime.datetime.now()\n",
    "    return_timestamp = str(current_timestamp.year) + \"{:02d}\".format(current_timestamp.month) + \"{:02d}\".format(current_timestamp.day) + \"_\" + \"{:02d}\".format(current_timestamp.hour) + \"{:02d}\".format(current_timestamp.minute)\n",
    "    # print(return_timestamp)\n",
    "    return return_timestamp\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Class for the experiment\n",
    "\"\"\"\n",
    "\n",
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Special class created to run the experiments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        argv_dict_dataframes={}, argv_df_intermediate=None,\n",
    "        argv_source=\"\", argv_destination=\"\", argv_weight=\"\", argv_path=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Setup the dictionaries and variables\n",
    "            argv_dict_dataframes is the dictionary of dataframes to be used for the experiments\n",
    "            argv_df_intermediate is the dataframe with a column for points to be used as intermediate vertices i nthe layerd graph\n",
    "            argv_source is the colname for the source of edges\n",
    "            argv_destination is the colname for the destination of edges\n",
    "            argv_weight is the list of colnames for columns to be used \n",
    "            argv_path is the variable to be used to generate the complete route/ path from the vertices\n",
    "        \"\"\"\n",
    "        # the dataset\n",
    "        self._dict_dataframes = argv_dict_dataframes\n",
    "        self._df_intermediate = argv_df_intermediate\n",
    "        # the graph details\n",
    "        self._source = \"\"\n",
    "        self._destination = \"\"\n",
    "        self._weight = [\"\"]\n",
    "        # got the pathing\n",
    "        self._path = \"\"\n",
    "\n",
    "    def load_dataframes(\n",
    "        self,\n",
    "        argv_filename_dataframes=\"\",\n",
    "        argv_id=None,\n",
    "        argv_colname_split=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Load the dataframe for the experiment into the attribute, it is a dictionary of dataframes\n",
    "            argv_filename_dataframes is the filename for the text document that list all of the dataframes filename\n",
    "            argv_id is the ID of the dataframe that would be used for the experiment\n",
    "            argv_colname_split is the colname of the column that would be used to \n",
    "        \"\"\"\n",
    "        manager_dataframe = Manager_Dataframe(argv_filename_dataframes)\n",
    "        manager_dataframe.load_dataframes(argv_dropNA=False)\n",
    "        manager_dataframe.split_dataframe(argv_id=argv_id, argv_col=argv_colname_split, argv_inplace=True)\n",
    "        self._dict_dataframes = manager_dataframe.get_dataframe()\n",
    "\n",
    "    def load_intermediate(\n",
    "        self,\n",
    "        argv_filename_dataframes=\"\",\n",
    "        argv_id=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Load the intermediate for the experiment into the attribute, it is a dataframe\n",
    "            argv_filename_dataframes is the filename for the text document that list all of the dataframes filename\n",
    "            argv_id is the ID of the dataframe that would be used for the intermediate points\n",
    "        \"\"\"\n",
    "        manager_intermediate = Manager_Dataframe(argv_filename_dataframes)\n",
    "        manager_intermediate.load_dataframes(argv_dropNA=False)\n",
    "        self._df_intermediate = manager_intermediate.get_dataframe(argv_id=argv_id)\n",
    "\n",
    "    def write_to_file(\n",
    "        self,\n",
    "        argv_content=\"\",\n",
    "        argv_filename=None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Function to append content to a file; could create a new file if the filename is provided\n",
    "            argv_content is the stirng content to be written\n",
    "            argv_filename is the filename for the content to be written\n",
    "                append if filename not provided\n",
    "        \"\"\"\n",
    "        if argv_filename is not None:\n",
    "            self._filename_write = argv_filename\n",
    "            self._writer = open(self._filename_write, \"w+\")\n",
    "            self._writer.close()\n",
    "        # open it as append\n",
    "        self._writer = open(self._filename_write, \"a+\")\n",
    "        self._writer.write(argv_content)\n",
    "        self._writer.close()\n",
    "\n",
    "    def generate_intermediates(\n",
    "        self,\n",
    "        argv_colname=\"POINT_nearest\",\n",
    "        argv_sizes=[],\n",
    "        argv_weights=[],\n",
    "        argv_unique=False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Function to generate the intermediate points\n",
    "            argv_colname is the colname of the DF to be used as random intermediate points\n",
    "            argv_sizes is a list of sizes to be used for the random list of intermediates\n",
    "            argv_weights is a list of weights to be used as weights for the itnermediate points\n",
    "                if list is empty, then the weights would be set to 0\n",
    "            argv_unique is the boolean control for the lists to have unique values between them\n",
    "        Return\n",
    "            return_intermediates is a list of lists with randomly selected points\n",
    "            return_intermediates_weight is a list of weights to be used as weight of intermediate nodes\n",
    "                Note at the moment we do not use different weights between intermediate points, this can be a future experiment to work on\n",
    "        \"\"\"\n",
    "        # the return as a list of list of points\n",
    "        return_intermediates = []\n",
    "        return_intermediates_weight = []\n",
    "        # get all the points to be used aas intermediate\n",
    "        current_points = self._df_intermediate[argv_colname].to_arrow().to_pylist()\n",
    "        # print(current_points)\n",
    "        # for each size, generate a random list of the size\n",
    "        return_intermediates = generate_random_lists(\n",
    "            argv_values=current_points,\n",
    "            argv_counts=argv_sizes,\n",
    "            argv_unique=argv_unique)\n",
    "        # for the weight\n",
    "        if len(argv_weights) == 0:\n",
    "            return_intermediates_weight = [0] * len(return_intermediates)\n",
    "        else:\n",
    "            return_intermediates_weight = argv_weights\n",
    "        # return it\n",
    "        return return_intermediates, return_intermediates_weight\n",
    "\n",
    "    def run_experiment_HERE(\n",
    "        self,\n",
    "        argv_filename_results=\"\",\n",
    "        argv_seed=1234,\n",
    "        argv_selected_starts = [],\n",
    "        argv_selected_ends = [],\n",
    "        argv_start_count=10,\n",
    "        argv_end_count=10,\n",
    "        argv_count_layers_max=5,\n",
    "        argv_size_layers=[],\n",
    "        argv_fullpath=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Main function hardcoded for the experiment\n",
    "            argv_filename_results is the filename for the results to be written\n",
    "            argv_seed is the seed that governs the randomness for the experiments\n",
    "            argv_selected_starts / argv_selected_ends are the lists of points to be used as the selected start and end points for the dijsktra\n",
    "                if left empty aka len()=0 then random points would be selected based on argv_start_count and argv_end_count\n",
    "            argv_start_count / argv_end_count is the number of randomly selected start/ end points\n",
    "            argv_count_layers_max is the maximum number of possible layers\n",
    "            argv_size_layers is a list of layer sizes\n",
    "                if left empty, then the sizes would be from 2**1 to 2**7\n",
    "            argv_fullpath is the boolean control if we want to output the full path for GIS plots.\n",
    "        \"\"\"\n",
    "        # set the seed\n",
    "        random.seed(argv_seed)\n",
    "\n",
    "        # intermediate details\n",
    "        current_count_layers_max = argv_count_layers_max\n",
    "        current_size_layers = argv_size_layers\n",
    "        if len(current_size_layers) == 0:\n",
    "            current_size_layers = []\n",
    "            for i in range(1,8,1):\n",
    "                current_size_layers.append(2**i)\n",
    "\n",
    "        # load the dataframe from the traffic data\n",
    "        filename_dfs = \"Listing_TrafficFlow_Cleaned.csv\"\n",
    "        current_id_df = \"Selangor_Cleaned\"\n",
    "        current_colname_split = \"PBT\"\n",
    "        self.load_dataframes(\n",
    "            argv_filename_dataframes=filename_dfs,\n",
    "            argv_id=current_id_df,\n",
    "            argv_colname_split = current_colname_split\n",
    "            )\n",
    "        # load the intermediate from selangor POI\n",
    "        filename_dfs = \"Listing_POI.csv\"\n",
    "        current_id_df = \"Selangor_Petrol\"\n",
    "        self.load_intermediate(\n",
    "            argv_filename_dataframes=filename_dfs,\n",
    "            argv_id=current_id_df\n",
    "        )\n",
    "\n",
    "        # set the graph details\n",
    "        self._source = \"start_point\"\n",
    "        self._destination = \"end_point\"\n",
    "        self._weight = [\"TMC_LE\"]\n",
    "        self._path = \"geometry\"\n",
    "\n",
    "        # setup writer for results and write to header\n",
    "        current_result_header = []\n",
    "        current_result_header.append(\"current_dataframeID\")\n",
    "        current_result_header.append(\"count_layers\")\n",
    "        current_result_header.append(\"size_layers\")\n",
    "        current_result_header.append(\"weight_layers\")\n",
    "        current_result_header.append(\"timer_layer_graph\")\n",
    "        current_result_header.append(\"current_point_start\")\n",
    "        current_result_header.append(\"current_point_end\")\n",
    "        current_result_header.append(\"timer_dijkstra\")\n",
    "        current_result_header.append(\"timer_pathbuilding\")\n",
    "        current_result_header.append(\"current_distance\")\n",
    "        current_result_header.append(\"length_current_path\")\n",
    "        current_result_header.append(\"detour\")\n",
    "        # current_result_header.append(\"detour_long\")\n",
    "        # current_result_header.append(\"detour_lat\")\n",
    "        if argv_fullpath:\n",
    "            current_result_header.append(\"full_path\")\n",
    "        # write the header\n",
    "        self.write_to_file(\n",
    "            argv_content=\",\".join(current_result_header),\n",
    "            argv_filename=argv_filename_results\n",
    "            )\n",
    "        \n",
    "        # count the cases\n",
    "        count_path_exist = 0\n",
    "        count_path_exist_not = 0\n",
    "\n",
    "        # loop dataframes\n",
    "        for current_key_dfs in self._dict_dataframes.keys():\n",
    "            # loop layer count\n",
    "            for current_count_layer in range(0,current_count_layers_max+1,1):\n",
    "                # loop layer size\n",
    "                for current_size_layer in current_size_layers:\n",
    "                    # get the dataframe\n",
    "                    current_df = self._dict_dataframes[current_key_dfs]\n",
    "                    # initialize the graph\n",
    "                    current_graph = Graph_Layered(\n",
    "                        argv_dataframe=current_df,\n",
    "                        argv_source=self._source, argv_destination=self._destination, argv_weight=self._weight\n",
    "                        )\n",
    "                    # get the edges stat\n",
    "                    current_weight_stats = current_graph.get_stat_weights()\n",
    "\n",
    "                    # select the start and end point\n",
    "                    # reset the seed\n",
    "                    random.seed(argv_seed)\n",
    "                    current_selected_starts = argv_selected_starts\n",
    "                    if len(current_selected_starts) == 0:\n",
    "                        current_selected_starts = select_random(argv_values=current_df[self._source].tolist(), argv_count=argv_start_count)\n",
    "                    current_selected_ends = argv_selected_ends\n",
    "                    if len(current_selected_ends) == 0:\n",
    "                        current_selected_ends = select_random(argv_values=current_df[self._destination].tolist(), argv_count=argv_end_count)\n",
    "\n",
    "                    # loop all weight types for intermediate layers\n",
    "                    for current_weight_stat in current_weight_stats.keys():\n",
    "                        # get the weight for the intermediate layers\n",
    "                        current_intermediate_weights = [current_weight_stats[current_weight_stat]] * current_count_layer\n",
    "                        # get the intermediate layers\n",
    "                        # reset the seed\n",
    "                        random.seed(argv_seed)\n",
    "                        current_intermediate_sizes = [current_size_layer] * current_count_layer\n",
    "                        current_intermediate_points, current_intermediate_weights = self.generate_intermediates(\n",
    "                            argv_colname=\"POINT_nearest\",\n",
    "                            argv_sizes=current_intermediate_sizes,\n",
    "                            argv_weights=current_intermediate_weights,\n",
    "                            argv_unique=False\n",
    "                            )\n",
    "                        # build the layered graph\n",
    "                        run_timer()\n",
    "                        current_graph.generate_layer_graph(\n",
    "                            argv_intermediates_vertices=current_intermediate_points,\n",
    "                            argv_intermediates_weights=current_intermediate_weights\n",
    "                            )\n",
    "                        timer_layer_graph = run_timer()\n",
    "                        # have the graph details, might be useful for more analysis on different graph types towards performance\n",
    "                        # we have our own generated graph maybe?\n",
    "                        # current_graph.print_graph_details()\n",
    "                        # build the dictionary route\n",
    "                        # this is needed to expand the points\n",
    "                        current_dict_routes = current_graph.build_dict_routes(argv_start=self._source, argv_end=self._destination, argv_geom=self._path)\n",
    "                        # for each start/end pair\n",
    "                        for current_point_start in current_selected_starts:\n",
    "                            for current_point_end in current_selected_ends:\n",
    "                                # skip if points the same\n",
    "                                if current_point_start == current_point_end:\n",
    "                                    continue\n",
    "                                # try out the pathing with dijkstra using the points\n",
    "                                # note, due to it being a digraph, path might not exist\n",
    "                                try:\n",
    "                                    # run dijkstra\n",
    "                                    # run_timer()\n",
    "                                    current_distance, current_path, timer_dijkstra, timer_pathbuilding = current_graph.run_dijsktra(argv_start=current_point_start, argv_end=current_point_end)\n",
    "                                    #print(current_distance, current_path)\n",
    "                                    # timer_dijkstra = run_timer()\n",
    "                                    # post-process dijkstra output\n",
    "                                    #print(current_distance)\n",
    "                                    if current_distance < sys.float_info.max:\n",
    "                                        current_path_cleaned, current_detour, current_detour_long, current_detour_lat = clean_layered_path(argv_path=current_path, argv_df_path=current_dict_routes)\n",
    "                                        # write the results\n",
    "                                        current_result_value = []\n",
    "                                        current_result_value.append(str(current_key_dfs))\n",
    "                                        current_result_value.append(str(current_count_layer))\n",
    "                                        current_result_value.append(str(current_size_layer))\n",
    "                                        current_result_value.append(str(current_weight_stat))\n",
    "                                        current_result_value.append(str(timer_layer_graph))\n",
    "                                        current_result_value.append(str(current_point_start))\n",
    "                                        current_result_value.append(str(current_point_end))\n",
    "                                        current_result_value.append(str(timer_dijkstra))\n",
    "                                        current_result_value.append(str(timer_pathbuilding))\n",
    "                                        current_result_value.append(str(current_distance))\n",
    "                                        current_result_value.append(str(len(current_path)))\n",
    "                                        current_result_value.append(\" \".join(current_detour))\n",
    "                                        # current_result_value.append(\" \".join(current_detour_long))\n",
    "                                        # current_result_value.append(\" \".join(current_detour_lat))\n",
    "\n",
    "                                        #print(current_result_value, 'current_result_value')\n",
    "\n",
    "                                        if argv_fullpath:\n",
    "                                            current_result_value.append(\" \".join(current_path_cleaned))\n",
    "                                        self.write_to_file(\n",
    "                                            argv_content=\"\\n\"\n",
    "                                            )\n",
    "                                        self.write_to_file(\n",
    "                                            argv_content=\",\".join(current_result_value),\n",
    "                                            )\n",
    "                                        count_path_exist += 1\n",
    "                                        #print(\"Path exist\")\n",
    "                                        #current_graph.print_graph_details()\n",
    "                                except Exception as e:\n",
    "                                    print(e)\n",
    "                                    #print('Something wrong')\n",
    "                                    count_path_exist_not += 1\n",
    "\n",
    "                    # no need to repeat the different layer sizes if there is no layer\n",
    "                    # this addition allow us to be more efficient\n",
    "                    if current_count_layer == 0:\n",
    "                        break\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Main driver for experiment\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    # create the experiment object\n",
    "    run_experiment = Experiment()\n",
    "    # generate the filename for the results\n",
    "    filename_output = \"Results_GPU_\" + get_current_timestamp() + \".csv\"\n",
    "    # run the experiment, values are hardcoded in the function already\n",
    "    run_experiment.run_experiment_HERE(argv_filename_results=filename_output)\n",
    "\n",
    "# %% ================================================================\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.float_info.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please wait a moment while I gather a list of all available modules...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/kernel/__init__.py:12: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n",
      "  warn(\"The `IPython.kernel` package has been deprecated since IPython 4.0.\"\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/cupyx/fallback_mode/__init__.py:10: FutureWarning: cupyx.fallback_mode.numpy is experimental. The interface can change in the future.\n",
      "  _util.experimental('cupyx.fallback_mode.numpy')\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/datashader/spatial/__init__.py:17: VisibleDeprecationWarning: The datashader.spatial module is deprecated as of version 0.11.0. The functionality it provided has migrated to the spatialpandas (github.com/holoviz/spatialpandas) and xarray-spatial (github.com/makepath/xarray-spatial) libraries.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/rapids/lib/python3.8/pkgutil.py:92: MatplotlibDeprecationWarning: \n",
      "The matplotlib.compat module was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "  __import__(info.name)\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/nltk/twitter/__init__.py:21: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/treelite/gallery/__init__.py:5: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  warnings.warn(('treelite.gallery.sklearn has been moved to treelite.sklearn. ' +\n",
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/treelite/gallery/sklearn/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  warnings.warn(('treelite.gallery.sklearn has been moved to treelite.sklearn. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython              cfnlint             json5               readline\n",
      "IPython             cgi                 jsondiff            readme_renderer\n",
      "Lib_TrafficFlow_Class_20210419_1648 cgitb               jsonpatch           recommonmark\n",
      "Lib_TrafficFlow_Class_20210419_1648_GPU chardet             jsonpickle          regex\n",
      "Lib_TrafficFlow_Class_20210425_1404_GPU chunk               jsonpointer         reprlib\n",
      "Lib_TrafficFlow_Class_20210427_1500_CPU cio                 jsonschema          requests\n",
      "OpenSSL             click               junit_xml           requests_oauthlib\n",
      "PIL                 click_plugins       jupyter             requests_toolbelt\n",
      "TCLIService         cligj               jupyter_client      resource\n",
      "__future__          cloudpickle         jupyter_core        responses\n",
      "_abc                cmake_setuptools    jupyter_server_proxy rfc3986\n",
      "_ast                cmarkgfm            jupyter_sphinx      rlcompleter\n",
      "_asyncio            cmath               jupyterlab          rmagic\n",
      "_bisect             cmd                 jupyterlab_nvdashboard rmm\n",
      "_black_version      code                jupyterlab_pygments rsa\n",
      "_blake2             codecs              jupyterlab_server   rtree\n",
      "_bootlocale         codeop              jwt                 ruamel_yaml\n",
      "_bz2                collections         keyring             runpy\n",
      "_cffi_backend       colorama            keyword             s3fs\n",
      "_codecs             colorcet            kiwisolver          s3transfer\n",
      "_codecs_cn          colorsys            lib2to3             samtranslator\n",
      "_codecs_hk          commonmark          libarchive          sasl\n",
      "_codecs_iso2022     community           libfuturize         sched\n",
      "_codecs_jp          compileall          libpasteurize       scipy\n",
      "_codecs_kr          concurrent          lief                seaborn\n",
      "_codecs_tw          conda               lightgbm            secrets\n",
      "_collections        conda_build         linecache           secretstorage\n",
      "_collections_abc    conda_env           llvmlite            select\n",
      "_compat_pickle      conda_package_handling locale              selectors\n",
      "_compression        conda_verify        locket              send2trash\n",
      "_contextvars        configparser        logging             setuptools\n",
      "_crypt              confluent_kafka     lzma                shapely\n",
      "_csv                contextlib          mailbox             shelve\n",
      "_ctypes             contextvars         mailcap             shlex\n",
      "_ctypes_test        cookies             main                shutil\n",
      "_curses             copy                markdown            signal\n",
      "_curses_panel       copyreg             markupsafe          simpervisor\n",
      "_datetime           coverage            marshal             site\n",
      "_decimal            cpuinfo             math                six\n",
      "_distutils_hack     crypt               matplotlib          sklearn\n",
      "_dummy_thread       cryptography        mccabe              smtpd\n",
      "_elementtree        csv                 mimesis             smtplib\n",
      "_functools          ctypes              mimetypes           sndhdr\n",
      "_hashlib            cudf                mistune             snowballstemmer\n",
      "_heapq              cudf_kafka          mmap                socket\n",
      "_imp                cugraph             mock                socketserver\n",
      "_io                 cuml                modulefinder        socks\n",
      "_jpype              cupy                more_itertools      sockshandler\n",
      "_json               cupy_backends       moto                sortedcontainers\n",
      "_locale             cupyx               msgpack             soupsieve\n",
      "_lsprof             curses              multidict           sphinx\n",
      "_lzma               cusignal            multipledispatch    sphinx_copybutton\n",
      "_markupbase         cuspatial           multiprocessing     sphinx_markdown_tables\n",
      "_md5                cuxfilter           munch               sphinx_rtd_theme\n",
      "_multibytecodec     cycler              mypy                spwd\n",
      "_multiprocessing    cython              mypy_extensions     sqlalchemy\n",
      "_opcode             cythonmagic         mypyc               sqlite3\n",
      "_operator           cytoolz             nbclient            sre_compile\n",
      "_osx_support        dask                nbconvert           sre_constants\n",
      "_pickle             dask_cuda           nbformat            sre_parse\n",
      "_posixshmem         dask_cudf           nbsphinx            sshpubkeys\n",
      "_posixsubprocess    dask_glm            nest_asyncio        ssl\n",
      "_py_abc             dask_labextension   netifaces           stat\n",
      "_pydecimal          dask_ml             netrc               statistics\n",
      "_pyio               dataclasses         networkx            statsmodels\n",
      "_pyrsistent_version datashader          nis                 storemagic\n",
      "_pytest             datashape           nltk                streamz\n",
      "_queue              datetime            nntplib             string\n",
      "_random             dateutil            notebook            stringprep\n",
      "_sha1               dbm                 ntpath              struct\n",
      "_sha256             decimal             nturl2path          subprocess\n",
      "_sha3               decorator           numba               sunau\n",
      "_sha512             defusedxml          numbergen           symbol\n",
      "_signal             difflib             numbers             sympyprinting\n",
      "_sitebuiltins       dis                 numpy               symtable\n",
      "_socket             distributed         numpydoc            sys\n",
      "_sqlite3            distutils           nvtx                sysconfig\n",
      "_sre                docker              oauthlib            syslog\n",
      "_ssl                dockerpycreds       ogr                 tabnanny\n",
      "_stat               doctest             olefile             tarfile\n",
      "_statistics         docutils            opcode              tblib\n",
      "_string             dummy_threading     operator            telnetlib\n",
      "_strptime           easy_install        optparse            tempfile\n",
      "_struct             ecdsa               os                  terminado\n",
      "_symtable           email               osgeo               termios\n",
      "_sysconfigdata__linux_x86_64-linux-gnu encodings           osr                 test\n",
      "_sysconfigdata_aarch64_conda_cos7_linux_gnu ensurepip           ossaudiodev         test_cookies\n",
      "_sysconfigdata_aarch64_conda_linux_gnu entrypoints         packaging           test_data\n",
      "_sysconfigdata_arm64_apple_darwin20_0_0 enum                pandas              test_pycosat\n",
      "_sysconfigdata_i686_conda_cos6_linux_gnu errno               pandocfilters       testpath\n",
      "_sysconfigdata_i686_conda_linux_gnu fa2                 panel               tests\n",
      "_sysconfigdata_powerpc64le_conda_cos7_linux_gnu fastavro            param               textwrap\n",
      "_sysconfigdata_powerpc64le_conda_linux_gnu fastrlock           parser              this\n",
      "_sysconfigdata_x86_64_apple_darwin13_4_0 faulthandler        parso               threading\n",
      "_sysconfigdata_x86_64_conda_cos6_linux_gnu fcntl               partd               threadpoolctl\n",
      "_sysconfigdata_x86_64_conda_linux_gnu feather             past                thrift\n",
      "_testbuffer         filecmp             pathlib             thrift_sasl\n",
      "_testcapi           fileinput           pathspec            time\n",
      "_testimportmultiple filelock            patsy               timeit\n",
      "_testinternalcapi   filterpy            pdb                 tkinter\n",
      "_testmultiphase     fiona               pexpect             tlz\n",
      "_thread             flake8              pickle              token\n",
      "_threading_local    flask               pickleshare         tokenize\n",
      "_tkinter            fnmatch             pickletools         toml\n",
      "_tracemalloc        folium              pip                 toolz\n",
      "_warnings           formatter           pipes               tornado\n",
      "_weakref            fractions           pkg_resources       tqdm\n",
      "_weakrefset         fsspec              pkginfo             trace\n",
      "_xxsubinterpreters  ftplib              pkgutil             traceback\n",
      "_xxtestfuzz         functools           platform            tracemalloc\n",
      "_yaml               future              plistlib            traitlets\n",
      "abc                 gc                  pluggy              treelite\n",
      "aifc                gcsfs               poplib              treelite_runtime\n",
      "aiohttp             gdal                posix               tty\n",
      "alabaster           gdalconst           posixpath           turtle\n",
      "antigravity         gdalnumeric         pprint              turtledemo\n",
      "app                 genericpath         profile             twine\n",
      "appdirs             geopandas           prometheus_client   typed_ast\n",
      "argon2              getopt              prompt_toolkit      types\n",
      "argparse            getpass             pstats              typing\n",
      "array               gettext             psutil              typing_extensions\n",
      "asn1crypto          glob                pty                 ucp\n",
      "ast                 glob2               ptyprocess          umap\n",
      "async_generator     google_auth_oauthlib pvectorc            unicodedata\n",
      "async_timeout       grp                 pwd                 unittest\n",
      "asynchat            gzip                py                  urllib\n",
      "asyncio             hashlib             py_compile          urllib3\n",
      "asyncore            heapdict            pyarrow             uu\n",
      "atexit              heapq               pyasn1              uuid\n",
      "attr                hmac                pyasn1_modules      venv\n",
      "audioop             holoviews           pyblazing           warnings\n",
      "autoreload          html                pyclbr              wave\n",
      "aws_xray_sdk        http                pycodestyle         wcwidth\n",
      "babel               httpretty           pycosat             weakref\n",
      "backcall            hypothesis          pycparser           webbrowser\n",
      "backports           idlelib             pyct                webencodings\n",
      "base64              idna                pydeck              websocket\n",
      "bdb                 imagesize           pydoc               websockets\n",
      "binascii            imaplib             pydoc_data          werkzeug\n",
      "binhex              imghdr              pyee                wheel\n",
      "bisect              imp                 pyexpat             widgetsnbextension\n",
      "black               importlib           pyflakes            wrapt\n",
      "blackd              importlib_metadata  pygments            wsgiref\n",
      "blazingsql          iniconfig           pyhive              xarray\n",
      "bleach              inspect             pylab               xdrlib\n",
      "blib2to3            io                  pynvml              xgboost\n",
      "blinker             ipaddress           pyparsing           xml\n",
      "bokeh               ipykernel           pyppeteer           xmlrpc\n",
      "boto                ipykernel_launcher  pyproj              xmltodict\n",
      "boto3               ipython_genutils    pyrsistent          xxlimited\n",
      "botocore            ipywidgets          pytest              xxsubtype\n",
      "branca              isort               pytest_asyncio      yaml\n",
      "brotli              itertools           pytest_benchmark    yarl\n",
      "bs4                 itsdangerous        pytest_cov          zict\n",
      "bsql_engine         jedi                pytest_timeout      zipapp\n",
      "builtins            jeepney             pytz                zipfile\n",
      "bz2                 jinja2              pyviz_comms         zipimport\n",
      "cProfile            jmespath            pyximport           zipp\n",
      "cachetools          joblib              queue               zlib\n",
      "calendar            jose                quopri              zmq\n",
      "certifi             jpype               random              \n",
      "cffi                json                re                  \n",
      "\n",
      "Enter any module name to get more help.  Or, type \"modules spam\" to search\n",
      "for modules whose name or summary contain the string \"spam\".\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/pkgutil.py:107: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.\n",
      "    Install tornado itself to use zmq with the tornado IOLoop.\n",
      "    \n",
      "  yield from walk_packages(path, info.name+'.', onerror)\n"
     ]
    }
   ],
   "source": [
    "help(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Import the libraries\n",
    "\"\"\"\n",
    "\n",
    "# simple python libraries\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import time\n",
    "# random\n",
    "import random\n",
    "# statistics\n",
    "import statistics\n",
    "\n",
    "# pandas for dataframes\n",
    "import pandas as pandas\n",
    "# shapes\n",
    "import shapely as shapely\n",
    "# graph\n",
    "import networkx as networkx\n",
    "\n",
    "# for map drawing\n",
    "import folium as folium\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Control variables\n",
    "\"\"\"\n",
    "FLAG_printDetail = False\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Dataframe manager class\n",
    "Manages the dataframes, allowing us to easily load dataframes for resuability\n",
    "\"\"\"\n",
    "\n",
    "class Manager_Dataframe:\n",
    "\n",
    "    def __init__(self, argv_filename=None):\n",
    "        \"\"\"\n",
    "        The init function store the name of the files to be loaded into dataframes\n",
    "            argv_filename is the filename of CSV data to be read into dataframes\n",
    "                if single filename (string) then it is a file that contain all filenames\n",
    "                if multiple filenames (dictionary) then it is already a dictionary of filenames\n",
    "        \"\"\"\n",
    "        # the attributes\n",
    "        self._filenames = {}\n",
    "        self._dataframes = {}\n",
    "        # check filenames are provided or not\n",
    "        if argv_filename is None:\n",
    "            pass\n",
    "        # if single filename, it is the file that contains all of the filenames\n",
    "        elif isinstance(argv_filename, str):\n",
    "            self._filenames = load_file(\n",
    "                argv_filename=argv_filename,\n",
    "                argv_type=\"dictionary\",\n",
    "                argv_separator=\",\"\n",
    "                )\n",
    "        # if it is a dictionary of filenames\n",
    "        elif isinstance(argv_filename, dict):\n",
    "            for current_key in argv_filename:\n",
    "                self._filenames[current_key] = argv_filename[current_key]\n",
    "\n",
    "    def load_dataframes(self, argv_dropNA=False):\n",
    "        \"\"\"\n",
    "        Load dataframe from the filenames, into as an attribute of the manager class\n",
    "            argv_dropNA is the flag to determine if missing values would be dropped wjhen the dataframe is read\n",
    "        \"\"\"\n",
    "        # check if there is data to load\n",
    "        if len(self._filenames) == 0:\n",
    "            return False\n",
    "        # load in the data\n",
    "        self._dataframes = {}\n",
    "        for current_fileID in self._filenames:\n",
    "            current_filename = self._filenames[current_fileID]\n",
    "            current_dataframe = load_into_dataframe(\n",
    "                argv_filename=current_filename,\n",
    "                argv_dropNA=argv_dropNA\n",
    "                )\n",
    "            self._dataframes[current_fileID] = current_dataframe\n",
    "\n",
    "    def get_dataframe(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Return the dataframe of choice\n",
    "            argv_id is the dataframe ID to retrieve\n",
    "                if left as None, retrieve all of the dataframes (dictionary)\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            return self._dataframes\n",
    "        if argv_id in self._dataframes:\n",
    "            return self._dataframes[argv_id]\n",
    "        return None\n",
    "\n",
    "    def print_filenames(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Print the filenames\n",
    "            argv_id is the dataframe ID to retrieve the filename which generate the dataframe\n",
    "                if left as None, print out all of the dataframe filenames\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            for current_id in self._filenames:\n",
    "                current_filename = self._filenames[current_id]\n",
    "                print(str(current_id) + \": \" + str(current_filename))\n",
    "        elif argv_id in self._filenames[argv_id]:\n",
    "            current_filename = self._filenames[argv_id]\n",
    "            print(str(argv_id) + \": \" + str(current_filename))\n",
    "\n",
    "    def print_dataframes(self, argv_id=None):\n",
    "        \"\"\"\n",
    "        Print the dataframe\n",
    "            argv_id is the dataframe ID to print out the details (summary information)\n",
    "                if left as None, would print out all of the dataframes information\n",
    "        \"\"\"\n",
    "        if argv_id is None:\n",
    "            for current_id in self._dataframes:\n",
    "                print(current_id)\n",
    "                current_dataframe = self._dataframes[current_id]\n",
    "                print(current_dataframe.info())\n",
    "                print(current_dataframe.head())\n",
    "        elif argv_id in self._dataframes:\n",
    "            print(argv_id)\n",
    "            current_dataframe = self._dataframes[argv_id]\n",
    "            print(current_dataframe.info())\n",
    "            print(current_dataframe.head())\n",
    "\n",
    "    def split_dataframe(self, argv_id=None, argv_col=\"\", argv_inplace=False):\n",
    "        \"\"\"\n",
    "        Split the single dataframe into multiple dataframes based on unique values in the column\n",
    "            argv_id is the dataframe ID to be splitted\n",
    "                if left as None, the last dataframe would be splitted (just being lazy for single dataframes)\n",
    "            argv_col is the column name where the datagrame would be splitted accordingly\n",
    "                split using the helper function split_dataframes_byCol()\n",
    "                will split based on unique value in the column\n",
    "            argv_inplace is the boolean value towards replacing all of the dataframes with the splitted one\n",
    "                default value is False\n",
    "        Returns True if process is successul\n",
    "        \"\"\"\n",
    "        # get the dataframe based on the ID\n",
    "        current_dataframe = None\n",
    "        if argv_id is None:\n",
    "            for current_id in self._dataframes:\n",
    "                current_dataframe = self._dataframes[current_id]\n",
    "        elif argv_id in self._dataframes:\n",
    "            current_dataframe = self._dataframes[argv_id]\n",
    "        else:\n",
    "            return False\n",
    "        # split the dataframe        \n",
    "        current_dfs = split_dataframes_byCol(argv_df=current_dataframe, argv_col=argv_col)\n",
    "        # check if it is in-place\n",
    "        if argv_inplace:\n",
    "            self._dataframes = current_dfs\n",
    "        else:\n",
    "            for current_id in current_dfs:\n",
    "                self._dataframes[current_id] = current_dfs[current_id]\n",
    "        # done\n",
    "        return True\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Graph class for layered graph\n",
    "\"\"\"\n",
    "\n",
    "class Graph_Layered:\n",
    "    \n",
    "    def __init__(self, argv_dataframe=None, argv_source=\"\", argv_destination=\"\", argv_weight=[]):\n",
    "        \"\"\"\n",
    "        Scope this down to only simple graph with single weight\n",
    "            argv_dataframe is the dataframe to be used for the layered graph\n",
    "            argv_source is the colname that would be used as the source of the edges for the graph\n",
    "            argv_destination is the colname that would be used as the destination of the edges for the graph\n",
    "            argv_weight is a list of colnames to be used as the weights for the edges of the graph\n",
    "                This is written to be more generic but in general we would be using single weighted edge\n",
    "        \"\"\"\n",
    "        # the attributes\n",
    "        self._dataframe = argv_dataframe\n",
    "        self._source = argv_source\n",
    "        self._destination = argv_destination\n",
    "        self._weight = argv_weight\n",
    "        self._graph = None\n",
    "        self._intermediates_vertices = [[]]\n",
    "        self._intermediates_weights = []\n",
    "\n",
    "    def get_stat_weights(self, argv_weight=None):\n",
    "        \"\"\"\n",
    "        Return the statistics for the edge weight\n",
    "        This is useful for the experiment variants of different weight values\n",
    "            argv_weight is the colname for the weight which the statistics is requested for\n",
    "        Return the statistics in a dictionary with keys\n",
    "            min\n",
    "            max\n",
    "            mean\n",
    "            median\n",
    "            zero\n",
    "        \"\"\"\n",
    "        # if no weight variable is selected then just use the first weight value\n",
    "        if argv_weight is None:\n",
    "            # get the stats\n",
    "            current_values = self._dataframe[self._weight[0]].tolist()\n",
    "            return_min = min(current_values)\n",
    "            return_max = max(current_values)\n",
    "            return_mean = sum(current_values) / max(len(current_values),1)\n",
    "            return_median = statistics.median(current_values)\n",
    "            # put the results in a dictionary\n",
    "            return_stats = {}\n",
    "            return_stats[\"min\"] = return_min\n",
    "            return_stats[\"max\"] = return_max\n",
    "            return_stats[\"mean\"] = return_mean\n",
    "            return_stats[\"median\"] = return_median\n",
    "            return_stats[\"zero\"] = 0\n",
    "        else:\n",
    "            # get the stats\n",
    "            current_values = self._dataframe[argv_weight].tolist()\n",
    "            return_min = min(current_values)\n",
    "            return_max = max(current_values)\n",
    "            return_mean = sum(current_values) / max(len(current_values),1)\n",
    "            return_median = statistics.median(current_values)\n",
    "            # put the results in a dictionary\n",
    "            return_stats = {}\n",
    "            return_stats[\"min\"] = return_min\n",
    "            return_stats[\"max\"] = return_max\n",
    "            return_stats[\"mean\"] = return_mean\n",
    "            return_stats[\"median\"] = return_median\n",
    "            return_stats[\"zero\"] = 0\n",
    "        return return_stats\n",
    "\n",
    "    def generate_graph(self, argv_directed=True):\n",
    "        \"\"\"\n",
    "        Generate a graph from the given dataframe, storing it within the graph object\n",
    "            argv_directed is the boolean to determine if the graph is directed or not\n",
    "        \"\"\"\n",
    "        self._graph = generate_graph(\n",
    "            argv_df=self._dataframe,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_edge=self._weight, \n",
    "            argv_directed=argv_directed)\n",
    "\n",
    "    def generate_layer_graph(self, argv_intermediates_vertices=[[]], argv_intermediates_weights=[]):\n",
    "        \"\"\"\n",
    "        Generate a layered graph from the dataframe, using the information stored within the attributes\n",
    "        The layered graph is generated through a dataframe approach (pandas) before being fed directly into networkx for the graph\n",
    "        Note graph is always directed for the layered graph approach\n",
    "            argv_intermediates_vertices is the list of vertices (vertex ID) to be used as intermediate vertices\n",
    "            argv_intermediates_weights is the list of weights (numeric) to be used for the edges connecting these intermediate vertices\n",
    "        \"\"\"\n",
    "        # update the attributes first\n",
    "        self._intermediates_vertices = argv_intermediates_vertices\n",
    "        self._intermediates_weights = argv_intermediates_weights\n",
    "        # generate the layers, store within dataframe\n",
    "        current_df_layered = generate_graph_layers_different(\n",
    "            argv_df=self._dataframe,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_weight=self._weight[0], \n",
    "            argv_intermediates_vertices=argv_intermediates_vertices, argv_intermediates_weights=argv_intermediates_weights,\n",
    "            argv_type=\"dataframe\"\n",
    "            )\n",
    "        # generate the graph\n",
    "        self._graph = generate_graph(\n",
    "            argv_df=current_df_layered,\n",
    "            argv_source=self._source, argv_destination=self._destination, argv_edge=self._weight, \n",
    "            argv_directed=True)\n",
    "\n",
    "    def print_graph_details(self):\n",
    "        \"\"\"\n",
    "        Simple function to print graph information, useful for future experiments when we update it to return values\n",
    "        \"\"\"\n",
    "        print(\"Nodes: \" + str(self._graph.number_of_nodes()))\n",
    "        print(\"Edges: \" + str(self._graph.number_of_edges()))\n",
    "        try:\n",
    "            print(\"Density: \" + str(networkx.density(self._graph)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def run_dijsktra(self, argv_start=\"\", argv_end=\"\"):\n",
    "        \"\"\"\n",
    "        Function that runs Dijkstra from the start to the end point, if route exist\n",
    "        Uses the run_dijkstra() helper function\n",
    "        Arguments\n",
    "            argv_start is the vertexID (string) for the starting point of path\n",
    "            argv_end is the vertexID (string) for the ending point of the path\n",
    "        Returns\n",
    "            return_distance return the total distance from the start to the end\n",
    "            return_path_nodes is the list of nodes (in the graph); not the actual route for the map because there is a detailed route between vertexA and vertexB\n",
    "        Throws exception if there is no route\n",
    "        \"\"\"\n",
    "        return_distance, return_path_nodes, timer_dijkstra, timer_pathbuilding = run_dijkstra(\n",
    "            argv_graph=self._graph,\n",
    "            argv_source=str(argv_start) + \"_0\",\n",
    "            argv_target=str(argv_end) + \"_\" + str(len(self._intermediates_vertices)),\n",
    "            argv_weight_edge=self._weight[0]\n",
    "        )\n",
    "        return return_distance, return_path_nodes, timer_dijkstra, timer_pathbuilding\n",
    "\n",
    "    def build_dict_routes(self, argv_start=\"\", argv_end=\"\", argv_geom=\"\"):\n",
    "        \"\"\"\n",
    "        Function that would build a dictionary of routes (the geom) from the dataframe\n",
    "        This dictionary would be useful in reconstructing the entire complete route from the path of points; in order to visualize it on the map.\n",
    "            argv_start is the colname of the source vertex ID in the graph\n",
    "            argv_end is the colname of the destination vertex ID in the graph\n",
    "            argv_geom is th colname where the geometry of the path from the source to the destination vertex\n",
    "        Return\n",
    "            dict_routes is the dictionary of routes with key = concat(start, \" \", end) and value is the list of path\n",
    "        \"\"\"\n",
    "        # get the values from the df\n",
    "        current_geometries = self._dataframe[argv_geom].tolist()\n",
    "        current_starts = self._dataframe[argv_start].tolist()\n",
    "        current_ends = self._dataframe[argv_end].tolist()\n",
    "        # add them into dictionary if unique\n",
    "        dict_routes = {}\n",
    "        for i in range(len(current_geometries)):\n",
    "            current_geometry = current_geometries[i]\n",
    "            current_start = current_starts[i]\n",
    "            current_end = current_ends[i]\n",
    "            # use this as \n",
    "            current_point = \" \".join([current_start, current_end])\n",
    "            # add to dictionary\n",
    "            if current_point not in dict_routes:\n",
    "                dict_routes[current_point] = current_geometry\n",
    "        # return the dictionary for use\n",
    "        return dict_routes\n",
    "\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "File IO\n",
    "\"\"\"\n",
    "\n",
    "#%% Set CWD\n",
    "def set_CWD():\n",
    "    \"\"\"\n",
    "    Change to CWD to where the file is located\n",
    "    This is generally not required except for certain Python setups\n",
    "    \"\"\"\n",
    "    abspath = os.path.abspath(__file__)\n",
    "    dname = os.path.dirname(abspath)\n",
    "    os.chdir(dname)\n",
    "    if FLAG_printDetail:\n",
    "        print(os.getcwd())\n",
    "\n",
    "def load_file(argv_filename=\"\", argv_type=\"list\", argv_separator=None):\n",
    "    \"\"\"\n",
    "    Load a file and return its content\n",
    "        argv_filename is the filename for the file to be loaded\n",
    "        argv_type (string) denote the datastructure that the file would be loaded into\n",
    "    Return\n",
    "        current_list is a list of content, each line is a list item\n",
    "        current_dictionary is a dictioanry of content, with the first token as the key and subsequent tokens as a string\n",
    "    \"\"\"\n",
    "    file_data = open(argv_filename, \"r\")\n",
    "    # load into list line by line, after striping\n",
    "    if argv_type == \"list\":\n",
    "        current_list = []\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_list.append(current_line)\n",
    "        file_data.close()\n",
    "        return current_list \n",
    "    # load into dictionary, with first split as the \n",
    "    if argv_type == \"dictionary\":\n",
    "        current_dictionary = {}\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            current_dictionary[current_line[0]] = \" \".join(current_line[1:])\n",
    "        file_data.close()\n",
    "        return current_dictionary\n",
    "\n",
    "def load_dataset(argv_filename=\"\", argv_separator=\",\", argv_type=\"dictionary\", argv_name=False):\n",
    "    \"\"\"\n",
    "    Function to load dataset into a different datatypes\n",
    "    This is a more complete function than the one above, but not being used as it is not needed\n",
    "    \"\"\"\n",
    "    file_data = open(argv_filename, \"r\")\n",
    "    if argv_type == \"dictionary\":\n",
    "        current_dictionary = {}\n",
    "        counter_line = 0\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            if argv_name is False:\n",
    "                current_dictionary[\"point_\"+str(counter_line)] = \" \".join(current_line)\n",
    "            else:\n",
    "                current_dictionary[current_line[0]] = \" \".join(current_line[1:])\n",
    "            counter_line += 1\n",
    "        file_data.close()\n",
    "        return current_dictionary\n",
    "    elif argv_type == \"list\":\n",
    "        current_list = []\n",
    "        for current_line in file_data:\n",
    "            current_line = current_line.strip()\n",
    "            if current_line == \"\":\n",
    "                continue\n",
    "            current_line = current_line.split(argv_separator)\n",
    "            current_list.append(\" \".join(current_line))\n",
    "        file_data.close()\n",
    "        return current_list\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Dataframes\n",
    "with pandas\n",
    "skip geopandas for now, as it isn't available on the server\n",
    "\"\"\"\n",
    "\n",
    "def load_into_dataframe(argv_filename=\"\", argv_pandas_type=None, argv_pandas_geomname=\"geometry\", argv_dropNA=True):\n",
    "    \"\"\"\n",
    "    Load the filename into a dataframe\n",
    "        argv_filename is the filename for the CSV file to be read into a Pandas DF\n",
    "        argv_pandas_type is the type of Pandas we would be using such as geopandas.\n",
    "            If None, then would be default panda.\n",
    "        argv_pandas_geomname is the colname for the column with the GIS data when using GeoPandas\n",
    "        argv_dropNA is the boolean variable to determine if missing values are dropped\n",
    "            Would use the default dropNA(), future extension here could provide more flexible drops\n",
    "    Would always skip blank lines by default.\n",
    "    Return\n",
    "        return_dataframe is the pandas DF object\n",
    "    \"\"\"\n",
    "    if argv_pandas_type is None:\n",
    "        return_dataframe = pandas.read_csv(filepath_or_buffer=argv_filename, skip_blank_lines=True)\n",
    "    elif argv_pandas_type == \"geopandas\":\n",
    "        # no need for geopandas, thus won't be included for now\n",
    "        # return_dataframe = geopandas.read_file(filename=argv_filename, GEOM_POSSIBLE_NAMES=argv_pandas_geomname, skip_blank_lines=True)\n",
    "        pass\n",
    "    # ignore missing (complete one by default)\n",
    "    if argv_dropNA:\n",
    "        return_dataframe = return_dataframe.dropna()\n",
    "    # return the dataframe\n",
    "    return return_dataframe\n",
    "\n",
    "def load_into_dataframes(argv_dict_filenames={}, argv_pandas_type=None, argv_pandas_geomname=\"geometry\", argv_dropNA=True):\n",
    "    \"\"\"\n",
    "    Load the filenames into a dictionary of dataframes\n",
    "        argv_dict_filenames is a dictionary of filenamnes to be loaded into the dataframe\n",
    "        argv_pandas_type is the type (string) of pandas to use (normal vs geopandas)\n",
    "        argv_pandas_geomname is the colname for the geometry column for geopandas\n",
    "        argv_dropNA is a boolean variable to control if rows with missing values would be dropped\n",
    "    Return\n",
    "        return_dict_dfs a dictionary of dataframes loaded from the files\n",
    "    \"\"\"\n",
    "    return_dict_dfs = {}\n",
    "    for current_key in argv_dict_filenames.keys():\n",
    "        current_filename = argv_dict_filenames[current_key]\n",
    "        return_dict_dfs[current_key] = load_into_dataframe(\n",
    "            argv_filename=current_filename,\n",
    "            argv_dropNA=argv_dropNA)\n",
    "    return return_dict_dfs\n",
    "\n",
    "def concat_dataframes(argv_dfs=[], argv_duplicates=True, argv_ignore_index=True):\n",
    "    \"\"\"\n",
    "    Concat dataframes together\n",
    "        argv_dfs is a list of dataframes\n",
    "        argv_duplicates check if duplicates are allowed, a boolean\n",
    "        argv_ignore_index is a boolean to ignore the index\n",
    "    Return\n",
    "        return_df the concatenated dataframe\n",
    "    \"\"\"\n",
    "    return_df = pandas.concat(argv_dfs, ignore_index=argv_ignore_index)\n",
    "    if not argv_duplicates:\n",
    "        return_df = return_df.drop_duplicates(keep=\"first\")\n",
    "    return return_df\n",
    "\n",
    "def split_dataframes_byCol(argv_df, argv_col=\"\"):\n",
    "    \"\"\"\n",
    "    Split a dataframe into multiple based on the unique values\n",
    "        argv_df is the dataframe object which would be used for splitting\n",
    "        argv_col is the column name in the dataframe of the object\n",
    "    Return\n",
    "        return_dfs a dictionary of Dataframes splitted based on their unique value (as the key for the dictionary as well)\n",
    "    \"\"\"\n",
    "    return_dfs = {}\n",
    "    # get unique values\n",
    "    current_values_unique = argv_df[argv_col].unique()\n",
    "    # loop through the unique values\n",
    "    for current_value in current_values_unique:\n",
    "        return_dfs[current_value] = argv_df.loc[argv_df[argv_col] == current_value]\n",
    "    return return_dfs\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Graph and graph layers\n",
    "\"\"\"\n",
    "\n",
    "def generate_graph(argv_df, argv_source=\"\", argv_destination=\"\", argv_edge=[], argv_directed=True):\n",
    "    \"\"\"\n",
    "    Generate a graph with the edges weighted according to a list of values from a dataframe\n",
    "        argv_df is the dataframe object where the graph would be generated\n",
    "        argv_source is the colname as the source for all of the edges in the graph\n",
    "        argv_destination is the colname as the destination for all of the edges in the graph\n",
    "        argv_edge is a list of colnames for edges value (multi edges are supported)\n",
    "        argv_directed is a boolean to determined if the fraph is directed.\n",
    "            Graph generated is directed by default for our use case.\n",
    "    Return\n",
    "        return_graph is a NetworkX graph object\n",
    "    \"\"\"\n",
    "    return_graph = None\n",
    "    if argv_directed:\n",
    "        return_graph = networkx.from_pandas_edgelist(\n",
    "            df=argv_df,\n",
    "            source=argv_source,\n",
    "            target=argv_destination,\n",
    "            edge_attr=argv_edge,\n",
    "            create_using=networkx.DiGraph()\n",
    "        )\n",
    "    else:\n",
    "        return_graph = networkx.from_pandas_edgelist(\n",
    "            df=argv_df,\n",
    "            source=argv_source,\n",
    "            target=argv_destination,\n",
    "            edge_attr=argv_edge\n",
    "        )\n",
    "    return return_graph\n",
    "\n",
    "def generate_graph_layers_different(\n",
    "    argv_df, argv_source=\"\", argv_destination=\"\", argv_weight=\"\", \n",
    "    argv_intermediates_vertices=[[]], argv_intermediates_weights=[],\n",
    "    argv_type=\"dataframe\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generated layered graph\n",
    "        argv_source/ argv_destination/ argv_weight are the edge details\n",
    "        argv_intermediates_vertices/ argv_intermediates_weights are array of\n",
    "            intermediate vertices [[v1,v2,v3], [v2,v5,v10,v11]]\n",
    "            weights connecting the intermediate nodes [5,2]\n",
    "        there should be at least 2 layers (1 intermediate link)\n",
    "        argv_type is how we do the layered, either via graph or dataframe\n",
    "            for now only dataframe as it is easier and more efficient\n",
    "    Return\n",
    "        return_df is the dataframe of the layered graph \n",
    "    \"\"\"\n",
    "    # for dataframe\n",
    "    if argv_type == \"dataframe\":\n",
    "        current_u = argv_df[argv_source].tolist()\n",
    "        current_v = argv_df[argv_destination].tolist()\n",
    "        current_w = argv_df[argv_weight].tolist()\n",
    "        current_u_new = []\n",
    "        current_v_new = []\n",
    "        current_w_new = []\n",
    "        # duplicate it for multiple layers first\n",
    "        current_count_layers = len(argv_intermediates_vertices)\n",
    "        # note: the +1 is needed as k intermediate layer would have k layers total\n",
    "        for count_layer in range(current_count_layers+1):\n",
    "            for count_row in range(len(current_u)):\n",
    "                current_u_new.append(str(current_u[count_row]) + \"_\" + str(count_layer))\n",
    "                current_v_new.append(str(current_v[count_row]) + \"_\" + str(count_layer))\n",
    "                current_w_new.append(float(current_w[count_row]))\n",
    "        # add in the intermediate laters\n",
    "        for count_layer in range(current_count_layers):\n",
    "            current_intermediate_vertices = argv_intermediates_vertices[count_layer]\n",
    "            current_intermediate_weight = argv_intermediates_weights[count_layer]\n",
    "            for current_intermeidate_vertex in current_intermediate_vertices:\n",
    "                current_u_new.append(str(current_intermeidate_vertex) + \"_\" + str(count_layer))\n",
    "                current_v_new.append(str(current_intermeidate_vertex) + \"_\" + str(count_layer+1))\n",
    "                current_w_new.append(float(current_intermediate_weight))\n",
    "        # create the df\n",
    "        return_df = pandas.DataFrame(\n",
    "            data=list(zip(current_u_new, current_v_new, current_w_new)),\n",
    "            columns=[argv_source, argv_destination, argv_weight]\n",
    "        )\n",
    "        return return_df\n",
    "    # return nothing when the wrong type is selected\n",
    "    return None\n",
    "\n",
    "def run_dijkstra(argv_graph, argv_source, argv_target, argv_weight_edge):\n",
    "    \"\"\"\n",
    "    Run dijkstra\n",
    "        argv_graph is the networkx graph to run Dijkstra on\n",
    "        argv_source and argv_target are vertex IDs\n",
    "        argv_weight_edge is the variable used for the graph weight in Dijkstra\n",
    "            Note graph could be multi edge, thus we would want to specific what is the weight we are being minimum\n",
    "    \"\"\"\n",
    "    #sssp_paths = networkx.single_source_shortest_path(\n",
    "    #   G=argv_graph,\n",
    "    #   source=argv_source\n",
    "    #)\n",
    "    \n",
    "    run_timer()\n",
    "    sssp_paths = networkx.dijkstra_predecessor_and_distance(\n",
    "        G=argv_graph,\n",
    "        source=argv_source,\n",
    "        weight=argv_weight_edge\n",
    "    )\n",
    "    timer_dijkstra = run_timer()\n",
    "    \n",
    "    run_timer()\n",
    "    \n",
    "    current_path = []\n",
    "    current_path.append(argv_target)\n",
    "    current_distance = sssp_paths[1][argv_target]\n",
    "    while argv_target != argv_source:\n",
    "        #print(argv_target, 'argv_target in loop')\n",
    "        argv_target = sssp_paths[0][argv_target][0] \n",
    "        current_path.append(argv_target)\n",
    "            \n",
    "    timer_pathbuilding = run_timer()\n",
    "    #print(current_distance, current_path, 'current distance + path')\n",
    "    \n",
    "    #current_distance, current_path = networkx.single_source_dijkstra(\n",
    "    #    G=argv_graph,\n",
    "    #    source=argv_source,\n",
    "    #    target=argv_target,\n",
    "    #    weight=argv_weight_edge\n",
    "    #)\n",
    "    \n",
    "    current_path.reverse()\n",
    "    \n",
    "    return current_distance, current_path, timer_dijkstra, timer_pathbuilding \n",
    "\n",
    "def clean_layered_path(argv_path, argv_df_path={}):\n",
    "    \"\"\"\n",
    "    Function to pre-processed the returned path from Dijkstra, completing the path for visualization on the map\n",
    "        argv_path is the path from Dijsktra through a layered graph; the vertices would have the \"_level\" in the ID\n",
    "        argv_df_path is the dictionary where the concat(start, \" \", end) as the key and the full path coordinates as the value\n",
    "    Return\n",
    "        return_path_full is a string of coordinates that acts as the full path of map plotting (with , as separator).\n",
    "        return_detour is a list of coordinates that are the detour points, this helps us mark the detour points on the map\n",
    "        return_detour_long/ return_detour_lat is the same, but just broken down for long and lat for visualization\n",
    "    \"\"\"\n",
    "    # clean\n",
    "    return_path = []\n",
    "    return_detour = []\n",
    "    return_detour_long = []\n",
    "    return_detour_lat = []\n",
    "    for current_point in argv_path:\n",
    "        current_point = current_point.split(\"_\")[0]\n",
    "        return_path.append(current_point)\n",
    "    # loop through\n",
    "    return_path_full = []\n",
    "    for i in range(len(return_path)-1):\n",
    "        current_point_start = return_path[i]\n",
    "        current_point_end = return_path[i+1]\n",
    "        # if start and end the same (ie moving through layers)\n",
    "        if current_point_start == current_point_end:\n",
    "            return_detour.append(current_point_start)\n",
    "            current_point_start_tokens = current_point_start.split()\n",
    "            return_detour_long.append(str(current_point_start_tokens[0]))\n",
    "            return_detour_lat.append(str(current_point_start_tokens[1]))\n",
    "            continue\n",
    "        # continue to the next one\n",
    "        current_endpoint = current_point_start + \" \" + current_point_end\n",
    "        if current_endpoint in argv_df_path:\n",
    "            current_path = argv_df_path[current_endpoint]\n",
    "            current_path = current_path.replace(\"], \", \"] , \")\n",
    "            current_path = current_path.split(\" , \")\n",
    "            current_path[0] = current_path[0].replace(\"[[\", \"[\")\n",
    "            # remove the end point, except for the last one as they do repeat\n",
    "            if i != len(return_path)-2:\n",
    "                current_path.pop(-1)\n",
    "            else:\n",
    "                current_path[-1] = current_path[-1].replace(\"]]\", \"]\")\n",
    "            # current_path = current_path.split(\", \")\n",
    "            # print(current_path)\n",
    "            return_path_full.extend(current_path)\n",
    "        else:\n",
    "            raise Exception(\"Error: No path\")\n",
    "    # return the values, concat the path\n",
    "    return_path_full = \", \".join(return_path_full)    \n",
    "    \n",
    "    return return_path_full, return_detour, return_detour_long, return_detour_lat\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Random functions\n",
    "Mainly used for evaluations\n",
    "\"\"\"\n",
    "\n",
    "def set_random_seed(argv_seed=1234):\n",
    "    \"\"\"\n",
    "    Simple function to reset the seeds\n",
    "        argv_seed is the seed value\n",
    "    \"\"\"\n",
    "    random.seed(argv_seed)\n",
    "\n",
    "def select_random(argv_values, argv_count=10):\n",
    "    \"\"\"\n",
    "    Randomly select from a list of values\n",
    "    Note that repetitions are not allowed\n",
    "        argv_values is a list of values to be selected\n",
    "        argv_count is the total number of items to be selected\n",
    "        precondition that argv_count <= argv_values\n",
    "    Return\n",
    "        return_selected is a list of randomly selected value\n",
    "    \"\"\"\n",
    "    return_selected = []\n",
    "    while len(return_selected) < argv_count:\n",
    "        current_value = random.choice(argv_values)\n",
    "        if current_value in return_selected:\n",
    "            continue\n",
    "        return_selected.append(current_value)\n",
    "    return return_selected\n",
    "\n",
    "def generate_random_lists(argv_values, argv_counts=[], argv_unique=False):\n",
    "    \"\"\"\n",
    "    From the provided list, generate multiple lists from the given sizes\n",
    "        argv_values is a list of values to be selected\n",
    "        argv_counts is a list of sizes of the lists to be generated\n",
    "        argv_unique is a boolean variable to ensure that the items in the randomly generated lists are all unique\n",
    "    Return\n",
    "        return_lists is a list of random lists generated based on the criteria\n",
    "    \"\"\"\n",
    "    return_lists = []\n",
    "    current_values = argv_values.copy()\n",
    "    # build the list for each size\n",
    "    for current_count in argv_counts:\n",
    "        # select random item based on size\n",
    "        current_list = select_random(argv_values=current_values, argv_count=current_count)\n",
    "        if argv_unique:\n",
    "            # then remove from original list\n",
    "            for current_item in current_list:\n",
    "                current_values.remove(current_item)\n",
    "        return_lists.append(current_list)\n",
    "    # return it\n",
    "    return return_lists\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Timer functions\n",
    "\"\"\"\n",
    "\n",
    "# import timer for benchmarking\n",
    "import time\n",
    "# have the start time as the global so we can make comparison\n",
    "START_TIME = time.time()\n",
    "def run_timer():\n",
    "    \"\"\"\n",
    "    Function that return the different in time between now and the previous time\n",
    "    Return\n",
    "        return_time is the time difference between now and the previous time\n",
    "    \"\"\"\n",
    "    global START_TIME\n",
    "    return_time = time.time() - START_TIME\n",
    "    START_TIME = time.time()\n",
    "    return return_time\n",
    "\n",
    "import datetime\n",
    "def get_current_timestamp():\n",
    "    \"\"\"\n",
    "    Gets the current time and format into a timestamp format\n",
    "    This is useful for auto-naming filename\n",
    "    Return\n",
    "        return_timestamp is the current timestamp formatted in YYYYMMDD_HHmm format\n",
    "    \"\"\"\n",
    "    current_timestamp = datetime.datetime.now()\n",
    "    return_timestamp = str(current_timestamp.year) + \"{:02d}\".format(current_timestamp.month) + \"{:02d}\".format(current_timestamp.day) + \"_\" + \"{:02d}\".format(current_timestamp.hour) + \"{:02d}\".format(current_timestamp.minute)\n",
    "    # print(return_timestamp)\n",
    "    return return_timestamp\n",
    "\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Class for the experiment\n",
    "\"\"\"\n",
    "\n",
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Special class created to run the experiments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        argv_dict_dataframes={}, argv_df_intermediate=None,\n",
    "        argv_source=\"\", argv_destination=\"\", argv_weight=\"\", argv_path=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Setup the dictionaries and variables\n",
    "            argv_dict_dataframes is the dictionary of dataframes to be used for the experiments\n",
    "            argv_df_intermediate is the dataframe with a column for points to be used as intermediate vertices i nthe layerd graph\n",
    "            argv_source is the colname for the source of edges\n",
    "            argv_destination is the colname for the destination of edges\n",
    "            argv_weight is the list of colnames for columns to be used \n",
    "            argv_path is the variable to be used to generate the complete route/ path from the vertices\n",
    "        \"\"\"\n",
    "        # the dataset\n",
    "        self._dict_dataframes = argv_dict_dataframes\n",
    "        self._df_intermediate = argv_df_intermediate\n",
    "        # the graph details\n",
    "        self._source = \"\"\n",
    "        self._destination = \"\"\n",
    "        self._weight = [\"\"]\n",
    "        # got the pathing\n",
    "        self._path = \"\"\n",
    "\n",
    "    def load_dataframes(\n",
    "        self,\n",
    "        argv_filename_dataframes=\"\",\n",
    "        argv_id=None,\n",
    "        argv_colname_split=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Load the dataframe for the experiment into the attribute, it is a dictionary of dataframes\n",
    "            argv_filename_dataframes is the filename for the text document that list all of the dataframes filename\n",
    "            argv_id is the ID of the dataframe that would be used for the experiment\n",
    "            argv_colname_split is the colname of the column that would be used to \n",
    "        \"\"\"\n",
    "        manager_dataframe = Manager_Dataframe(argv_filename_dataframes)\n",
    "        manager_dataframe.load_dataframes(argv_dropNA=False)\n",
    "        manager_dataframe.split_dataframe(argv_id=argv_id, argv_col=argv_colname_split, argv_inplace=True)\n",
    "        self._dict_dataframes = manager_dataframe.get_dataframe()\n",
    "\n",
    "    def load_intermediate(\n",
    "        self,\n",
    "        argv_filename_dataframes=\"\",\n",
    "        argv_id=\"\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Load the intermediate for the experiment into the attribute, it is a dataframe\n",
    "            argv_filename_dataframes is the filename for the text document that list all of the dataframes filename\n",
    "            argv_id is the ID of the dataframe that would be used for the intermediate points\n",
    "        \"\"\"\n",
    "        manager_intermediate = Manager_Dataframe(argv_filename_dataframes)\n",
    "        manager_intermediate.load_dataframes(argv_dropNA=False)\n",
    "        self._df_intermediate = manager_intermediate.get_dataframe(argv_id=argv_id)\n",
    "\n",
    "    def write_to_file(\n",
    "        self,\n",
    "        argv_content=\"\",\n",
    "        argv_filename=None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Function to append content to a file; could create a new file if the filename is provided\n",
    "            argv_content is the stirng content to be written\n",
    "            argv_filename is the filename for the content to be written\n",
    "                append if filename not provided\n",
    "        \"\"\"\n",
    "        if argv_filename is not None:\n",
    "            self._filename_write = argv_filename\n",
    "            self._writer = open(self._filename_write, \"w+\")\n",
    "            self._writer.close()\n",
    "        # open it as append\n",
    "        self._writer = open(self._filename_write, \"a+\")\n",
    "        self._writer.write(argv_content)\n",
    "        self._writer.close()\n",
    "\n",
    "    def generate_intermediates(\n",
    "        self,\n",
    "        argv_colname=\"POINT_nearest\",\n",
    "        argv_sizes=[],\n",
    "        argv_weights=[],\n",
    "        argv_unique=False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Function to generate the intermediate points\n",
    "            argv_colname is the colname of the DF to be used as random intermediate points\n",
    "            argv_sizes is a list of sizes to be used for the random list of intermediates\n",
    "            argv_weights is a list of weights to be used as weights for the itnermediate points\n",
    "                if list is empty, then the weights would be set to 0\n",
    "            argv_unique is the boolean control for the lists to have unique values between them\n",
    "        Return\n",
    "            return_intermediates is a list of lists with randomly selected points\n",
    "            return_intermediates_weight is a list of weights to be used as weight of intermediate nodes\n",
    "                Note at the moment we do not use different weights between intermediate points, this can be a future experiment to work on\n",
    "        \"\"\"\n",
    "        # the return as a list of list of points\n",
    "        return_intermediates = []\n",
    "        return_intermediates_weight = []\n",
    "        # get all the points to be used aas intermediate\n",
    "        current_points = self._df_intermediate[argv_colname].tolist()\n",
    "        # print(current_points)\n",
    "        # for each size, generate a random list of the size\n",
    "        return_intermediates = generate_random_lists(\n",
    "            argv_values=current_points,\n",
    "            argv_counts=argv_sizes,\n",
    "            argv_unique=argv_unique)\n",
    "        # for the weight\n",
    "        if len(argv_weights) == 0:\n",
    "            return_intermediates_weight = [0] * len(return_intermediates)\n",
    "        else:\n",
    "            return_intermediates_weight = argv_weights\n",
    "        # return it\n",
    "        return return_intermediates, return_intermediates_weight\n",
    "\n",
    "    def run_experiment_HERE(\n",
    "        self,\n",
    "        argv_filename_results=\"\",\n",
    "        argv_seed=1234,\n",
    "        argv_selected_starts = [],\n",
    "        argv_selected_ends = [],\n",
    "        argv_start_count=10,\n",
    "        argv_end_count=10,\n",
    "        argv_count_layers_max=5,\n",
    "        argv_size_layers=[],\n",
    "        argv_fullpath=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Main function hardcoded for the experiment\n",
    "            argv_filename_results is the filename for the results to be written\n",
    "            argv_seed is the seed that governs the randomness for the experiments\n",
    "            argv_selected_starts / argv_selected_ends are the lists of points to be used as the selected start and end points for the dijsktra\n",
    "                if left empty aka len()=0 then random points would be selected based on argv_start_count and argv_end_count\n",
    "            argv_start_count / argv_end_count is the number of randomly selected start/ end points\n",
    "            argv_count_layers_max is the maximum number of possible layers\n",
    "            argv_size_layers is a list of layer sizes\n",
    "                if left empty, then the sizes would be from 2**1 to 2**7\n",
    "            argv_fullpath is the boolean control if we want to output the full path for GIS plots.\n",
    "        \"\"\"\n",
    "        # set the seed\n",
    "        random.seed(argv_seed)\n",
    "\n",
    "        # intermediate details\n",
    "        current_count_layers_max = argv_count_layers_max\n",
    "        current_size_layers = argv_size_layers\n",
    "        if len(current_size_layers) == 0:\n",
    "            current_size_layers = []\n",
    "            for i in range(1,8,1):\n",
    "                current_size_layers.append(2**i)\n",
    "\n",
    "        # load the dataframe from the traffic data\n",
    "        filename_dfs = \"Listing_TrafficFlow_Cleaned.csv\"\n",
    "        current_id_df = \"Selangor_Cleaned\"\n",
    "        current_colname_split = \"PBT\"\n",
    "        self.load_dataframes(\n",
    "            argv_filename_dataframes=filename_dfs,\n",
    "            argv_id=current_id_df,\n",
    "            argv_colname_split = current_colname_split\n",
    "            )\n",
    "        # load the intermediate from selangor POI\n",
    "        filename_dfs = \"Listing_POI.csv\"\n",
    "        current_id_df = \"Selangor_Petrol\"\n",
    "        self.load_intermediate(\n",
    "            argv_filename_dataframes=filename_dfs,\n",
    "            argv_id=current_id_df\n",
    "        )\n",
    "\n",
    "        # set the graph details\n",
    "        self._source = \"start_point\"\n",
    "        self._destination = \"end_point\"\n",
    "        self._weight = [\"TMC_LE\"]\n",
    "        self._path = \"geometry\"\n",
    "\n",
    "        # setup writer for results and write to header\n",
    "        current_result_header = []\n",
    "        current_result_header.append(\"current_dataframeID\")\n",
    "        current_result_header.append(\"count_layers\")\n",
    "        current_result_header.append(\"size_layers\")\n",
    "        current_result_header.append(\"weight_layers\")\n",
    "        current_result_header.append(\"timer_layer_graph\")\n",
    "        current_result_header.append(\"current_point_start\")\n",
    "        current_result_header.append(\"current_point_end\")\n",
    "        current_result_header.append(\"timer_dijkstra\")\n",
    "        current_result_header.append(\"timer_pathbuilding\")\n",
    "        current_result_header.append(\"current_distance\")\n",
    "        current_result_header.append(\"length_current_path\")\n",
    "        current_result_header.append(\"detour\")\n",
    "        # current_result_header.append(\"detour_long\")\n",
    "        # current_result_header.append(\"detour_lat\")\n",
    "        if argv_fullpath:\n",
    "            current_result_header.append(\"full_path\")\n",
    "        # write the header\n",
    "        self.write_to_file(\n",
    "            argv_content=\",\".join(current_result_header),\n",
    "            argv_filename=argv_filename_results\n",
    "            )\n",
    "        \n",
    "        # count the cases\n",
    "        count_path_exist = 0\n",
    "        count_path_exist_not = 0\n",
    "\n",
    "        # loop dataframes\n",
    "        for current_key_dfs in self._dict_dataframes.keys():\n",
    "            # loop layer count\n",
    "            for current_count_layer in range(0,current_count_layers_max+1,1):\n",
    "                # loop layer size\n",
    "                for current_size_layer in current_size_layers:\n",
    "                    # get the dataframe\n",
    "                    current_df = self._dict_dataframes[current_key_dfs]\n",
    "                    # initialize the graph\n",
    "                    current_graph = Graph_Layered(\n",
    "                        argv_dataframe=current_df,\n",
    "                        argv_source=self._source, argv_destination=self._destination, argv_weight=self._weight\n",
    "                        )\n",
    "                    # get the edges stat\n",
    "                    current_weight_stats = current_graph.get_stat_weights()\n",
    "\n",
    "                    # select the start and end point\n",
    "                    # reset the seed\n",
    "                    random.seed(argv_seed)\n",
    "                    current_selected_starts = argv_selected_starts\n",
    "                    if len(current_selected_starts) == 0:\n",
    "                        current_selected_starts = select_random(argv_values=current_df[self._source].tolist(), argv_count=argv_start_count)\n",
    "                    current_selected_ends = argv_selected_ends\n",
    "                    if len(current_selected_ends) == 0:\n",
    "                        current_selected_ends = select_random(argv_values=current_df[self._destination].tolist(), argv_count=argv_end_count)\n",
    "\n",
    "                    # loop all weight types for intermediate layers\n",
    "                    for current_weight_stat in current_weight_stats.keys():\n",
    "                        # get the weight for the intermediate layers\n",
    "                        current_intermediate_weights = [current_weight_stats[current_weight_stat]] * current_count_layer\n",
    "                        # get the intermediate layers\n",
    "                        # reset the seed\n",
    "                        random.seed(argv_seed)\n",
    "                        current_intermediate_sizes = [current_size_layer] * current_count_layer\n",
    "                        current_intermediate_points, current_intermediate_weights = self.generate_intermediates(\n",
    "                            argv_colname=\"POINT_nearest\",\n",
    "                            argv_sizes=current_intermediate_sizes,\n",
    "                            argv_weights=current_intermediate_weights,\n",
    "                            argv_unique=False\n",
    "                            )\n",
    "                        # build the layered graph\n",
    "                        run_timer()\n",
    "                        current_graph.generate_layer_graph(\n",
    "                            argv_intermediates_vertices=current_intermediate_points,\n",
    "                            argv_intermediates_weights=current_intermediate_weights\n",
    "                            )\n",
    "                        timer_layer_graph = run_timer()\n",
    "                        # have the graph details, might be useful for more analysis on different graph types towards performance\n",
    "                        # we have our own generated graph maybe?\n",
    "                        # current_graph.print_graph_details()\n",
    "                        # build the dictionary route\n",
    "                        # this is needed to expand the points\n",
    "                        current_dict_routes = current_graph.build_dict_routes(argv_start=self._source, argv_end=self._destination, argv_geom=self._path)\n",
    "\n",
    "                        # for each start/end pair\n",
    "                        for current_point_start in current_selected_starts:\n",
    "                            for current_point_end in current_selected_ends:\n",
    "                                # skip if points the same\n",
    "                                if current_point_start == current_point_end:\n",
    "                                    continue\n",
    "                                # try out the pathing with dijkstra using the points\n",
    "                                # note, due to it being a digraph, path might not exist\n",
    "                                try:\n",
    "                                    # run dijkstra\n",
    "                                    # run_timer()\n",
    "                                    current_distance, current_path, timer_dijkstra, timer_pathbuilding = current_graph.run_dijsktra(argv_start=current_point_start, argv_end=current_point_end)\n",
    "                                    timer_dijkstra = run_timer()\n",
    "                                    # post-process dijkstra output               \n",
    "                                    current_path_cleaned, current_detour, current_detour_long, current_detour_lat = clean_layered_path(argv_path=current_path, argv_df_path=current_dict_routes)\n",
    "                                    # write the results\n",
    "                                    current_result_value = []\n",
    "                                    current_result_value.append(str(current_key_dfs))\n",
    "                                    current_result_value.append(str(current_count_layer))\n",
    "                                    current_result_value.append(str(current_size_layer))\n",
    "                                    current_result_value.append(str(current_weight_stat))\n",
    "                                    current_result_value.append(str(timer_layer_graph))\n",
    "                                    current_result_value.append(str(current_point_start))\n",
    "                                    current_result_value.append(str(current_point_end))\n",
    "                                    current_result_value.append(str(timer_dijkstra))\n",
    "                                    current_result_value.append(str(timer_pathbuilding))\n",
    "                                    current_result_value.append(str(current_distance))\n",
    "                                    current_result_value.append(str(len(current_path)))\n",
    "                                    current_result_value.append(\" \".join(current_detour))\n",
    "                                    # current_result_value.append(\" \".join(current_detour_long))\n",
    "                                    # current_result_value.append(\" \".join(current_detour_lat))\n",
    "                                    #print(current_result_value)\n",
    "                                    if argv_fullpath:\n",
    "                                        current_result_value.append(\" \".join(current_path_cleaned))\n",
    "                                    self.write_to_file(\n",
    "                                        argv_content=\"\\n\"\n",
    "                                        )\n",
    "                                    self.write_to_file(\n",
    "                                        argv_content=\",\".join(current_result_value),\n",
    "                                        )\n",
    "                                    count_path_exist += 1\n",
    "                                    # print(\"Path exist\")\n",
    "                                    # current_graph.print_graph_details()\n",
    "                                except Exception as e:\n",
    "                                    #print(e)\n",
    "                                    count_path_exist_not += 1\n",
    "\n",
    "                    # no need to repeat the different layer sizes if there is no layer\n",
    "                    # this addition allow us to be more efficient\n",
    "                    if current_count_layer == 0:\n",
    "                        break\n",
    "\n",
    "#%% ================================================================\n",
    "\"\"\"\n",
    "Main driver for experiment\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    # create the experiment object\n",
    "    run_experiment = Experiment()\n",
    "    # generate the filename for the results\n",
    "    filename_output = \"Results_CPU_\" + get_current_timestamp() + \".csv\"\n",
    "    # run the experiment, values are hardcoded in the function already\n",
    "    run_experiment.run_experiment_HERE(argv_filename_results=filename_output)\n",
    "\n",
    "# %% ================================================================\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
